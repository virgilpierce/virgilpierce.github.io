<!DOCTYPE html>
<!--**************************************-->
<!--*    Generated from PreTeXt source   *-->
<!--*    on 2019-11-19T13:28:10-07:00    *-->
<!--*                                    *-->
<!--*      https://pretextbook.org       *-->
<!--*                                    *-->
<!--**************************************-->
<html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Dealing with Categorical Data</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width,  initial-scale=1.0, user-scalable=0, minimum-scale=1.0, maximum-scale=1.0">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['\\(','\\)']]
    },
    asciimath2jax: {
        ignoreClass: ".*",
        processClass: "has_am"
    },
    jax: ["input/AsciiMath"],
    extensions: ["asciimath2jax.js"],
    TeX: {
        extensions: ["extpfeil.js", "autobold.js", "https://pretextbook.org/js/lib/mathjaxknowl.js", ],
        // scrolling to fragment identifiers is controlled by other Javascript
        positionToHash: false,
        equationNumbers: { autoNumber: "none", useLabelIds: true, },
        TagSide: "right",
        TagIndent: ".8em",
    },
    // HTML-CSS output Jax to be dropped for MathJax 3.0
    "HTML-CSS": {
        scale: 88,
        mtextFontInherit: true,
    },
    CommonHTML: {
        scale: 88,
        mtextFontInherit: true,
    },
});
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML-full"></script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.12/pretext.js"></script><script src="https://pretextbook.org/js/0.12/pretext_add_on.js"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/toc.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/colors_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/setcolors.css" rel="stylesheet" type="text/css">
<!-- 2019-10-12: Temporary - CSS file for experiments with styling --><link href="developer.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/features.css" rel="stylesheet" type="text/css">
<script>var logged_in = false;
var role = 'student';
var guest_access = true;
var login_required = false;
var js_version = 0.12;
</script>
</head>
<body class="mathbook-book has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div class="hidden-content" style="display:none">\(\newcommand{\doubler}[1]{2#1}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\)</div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="datascience.html"><span class="title">Data Science with Python</span></a></h1>
<p class="byline">Virgil U Pierce</p>
</div>
</div></div>
<nav id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3"><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="section-35.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="wrangling.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="section-37.html" title="Next">Next</a></span></div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="section-35.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="wrangling.html" title="Up">Up</a><a class="next-button button toolbar-item" href="section-37.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link"><a href="frontmatter-1.html" data-scroll="frontmatter-1"><span class="title">Front Matter</span></a></li>
<li class="link">
<a href="course_syllabus.html" data-scroll="course_syllabus"><span class="codenumber">1</span> <span class="title">Course Syllabus</span></a><ul>
<li><a href="section-1.html" data-scroll="section-1">Class and Instructor Details</a></li>
<li><a href="section-2.html" data-scroll="section-2">Course Description</a></li>
<li><a href="section-3.html" data-scroll="section-3">Textbook and Software</a></li>
<li><a href="section-4.html" data-scroll="section-4">Learning Objectives / Outcomes for the Course</a></li>
<li><a href="section-5.html" data-scroll="section-5">Communicating</a></li>
<li><a href="section-6.html" data-scroll="section-6">Course Outline</a></li>
<li><a href="section-7.html" data-scroll="section-7">Connections with Industrial Mathematics</a></li>
<li><a href="section-8.html" data-scroll="section-8">Assessments</a></li>
<li><a href="section-9.html" data-scroll="section-9">UNCO Policy Statements</a></li>
</ul>
</li>
<li class="link">
<a href="data_science_introduction.html" data-scroll="data_science_introduction"><span class="codenumber">2</span> <span class="title">Introduction to Data Science</span></a><ul>
<li><a href="section-10.html" data-scroll="section-10">Anaconda, Jupyter, and Python</a></li>
<li><a href="section-11.html" data-scroll="section-11">Github</a></li>
<li><a href="section-12.html" data-scroll="section-12">Python</a></li>
<li><a href="section-13.html" data-scroll="section-13">Some Prelimaries</a></li>
<li><a href="section-14.html" data-scroll="section-14">First Motivating Example - Baseball Players</a></li>
<li><a href="section-15.html" data-scroll="section-15">Second Motivating Example - Abalone Characteristics</a></li>
<li><a href="section-16.html" data-scroll="section-16">Third Motivating Example - US Income Levels</a></li>
<li><a href="section-17.html" data-scroll="section-17">Fourth Motivating Example - Mushroom Characteristics</a></li>
<li><a href="section-18.html" data-scroll="section-18">Fifth Motivating Example - House Prices</a></li>
<li><a href="section-19.html" data-scroll="section-19">Sixth Motivating Example - Running Data from Garmin</a></li>
<li><a href="section-20.html" data-scroll="section-20">Seventh Motivating Example - Berlin Airbnb Data</a></li>
<li><a href="section-21.html" data-scroll="section-21">Eighth Motivating Example - Colorado Child Care</a></li>
<li><a href="section-22.html" data-scroll="section-22">Ninth Motivating Example - Flight Delays at DEN</a></li>
<li><a href="section-23.html" data-scroll="section-23">Tenth Motivating Example - Image Classification</a></li>
<li><a href="section-24.html" data-scroll="section-24">Left for a future class - Unsupervised Learning</a></li>
</ul>
</li>
<li class="link">
<a href="data.html" data-scroll="data"><span class="codenumber">3</span> <span class="title">Data</span></a><ul>
<li><a href="section-25.html" data-scroll="section-25">What is Data</a></li>
<li><a href="section-26.html" data-scroll="section-26">Supervised versus Unsupervised Learning</a></li>
<li><a href="section-27.html" data-scroll="section-27">Where to get Data</a></li>
</ul>
</li>
<li class="link">
<a href="tools.html" data-scroll="tools"><span class="codenumber">4</span> <span class="title">Tools of the Trade</span></a><ul>
<li><a href="section-28.html" data-scroll="section-28">Python and Jupyter</a></li>
<li><a href="section-29.html" data-scroll="section-29">Development</a></li>
<li><a href="section-30.html" data-scroll="section-30">Versioning Control</a></li>
</ul>
</li>
<li class="link">
<a href="process.html" data-scroll="process"><span class="codenumber">5</span> <span class="title">The Data Science Process</span></a><ul>
<li><a href="section-31.html" data-scroll="section-31">Professional Ethics</a></li>
<li><a href="section-32.html" data-scroll="section-32">Controlling for Error</a></li>
<li><a href="section-33.html" data-scroll="section-33">Error in Categorization Problems</a></li>
</ul>
</li>
<li class="link">
<a href="wrangling.html" data-scroll="wrangling"><span class="codenumber">6</span> <span class="title">Wrangling the Data</span></a><ul>
<li><a href="section-34.html" data-scroll="section-34">Formatting the Data</a></li>
<li><a href="section-35.html" data-scroll="section-35">Dealing with Strings</a></li>
<li><a href="section-36.html" data-scroll="section-36" class="active">Dealing with Categorical Data</a></li>
<li><a href="section-37.html" data-scroll="section-37">Dealing with Missing Data</a></li>
<li><a href="section-38.html" data-scroll="section-38">Dealing with Images</a></li>
</ul>
</li>
<li class="link">
<a href="resampling.html" data-scroll="resampling"><span class="codenumber">7</span> <span class="title">Resampling</span></a><ul>
<li><a href="section-39.html" data-scroll="section-39">Cross Validation</a></li>
<li><a href="section-40.html" data-scroll="section-40">Bootstraps</a></li>
</ul>
</li>
<li class="link">
<a href="EDA.html" data-scroll="EDA"><span class="codenumber">8</span> <span class="title">Exploratory Data Analysis</span></a><ul><li><a href="section-41.html" data-scroll="section-41">Nonlinear Relations</a></li></ul>
</li>
<li class="link">
<a href="linear_regression.html" data-scroll="linear_regression"><span class="codenumber">9</span> <span class="title">Linear Regression</span></a><ul>
<li><a href="section-42.html" data-scroll="section-42">Calculus Approach to Linear Regression</a></li>
<li><a href="section-43.html" data-scroll="section-43">Linear Regression as Projection</a></li>
</ul>
</li>
<li class="link">
<a href="pca.html" data-scroll="pca"><span class="codenumber">10</span> <span class="title">Principal Component Analysis</span></a><ul>
<li><a href="section-44.html" data-scroll="section-44">Eigenvalue Decomposition of Square Matrix</a></li>
<li><a href="section-45.html" data-scroll="section-45">Singular Value Decomposition</a></li>
</ul>
</li>
<li class="link">
<a href="k-nn.html" data-scroll="k-nn"><span class="codenumber">11</span> <span class="title">k-Nearest Neighbors</span></a><ul>
<li><a href="section-46.html" data-scroll="section-46">Checking Performance with Bootstraps</a></li>
<li><a href="section-47.html" data-scroll="section-47">Normalization</a></li>
<li><a href="section-48.html" data-scroll="section-48">k-Nearest Neighbors with Many Factors</a></li>
<li><a href="section-49.html" data-scroll="section-49">k-Nearest Neighbors for Regression</a></li>
</ul>
</li>
<li class="link">
<a href="ridge_and_lasso.html" data-scroll="ridge_and_lasso"><span class="codenumber">12</span> <span class="title">Ridge and Lasso Regression</span></a><ul>
<li><a href="section-50.html" data-scroll="section-50">House Pricing Data: Linear Regression</a></li>
<li><a href="section-51.html" data-scroll="section-51">Ridge Regression</a></li>
<li><a href="section-52.html" data-scroll="section-52">Lasso Regression</a></li>
</ul>
</li>
<li class="link">
<a href="lda_svm.html" data-scroll="lda_svm"><span class="codenumber">13</span> <span class="title">Linear Discrimant Analysis and Support Vector Machines</span></a><ul>
<li><a href="section-53.html" data-scroll="section-53">Linear Discriminant Analysis</a></li>
<li><a href="section-54.html" data-scroll="section-54">Support Vector Machines</a></li>
</ul>
</li>
<li class="link">
<a href="decision_trees.html" data-scroll="decision_trees"><span class="codenumber">14</span> <span class="title">Decsion Trees</span></a><ul>
<li><a href="section-55.html" data-scroll="section-55">Regression Trees</a></li>
<li><a href="section-56.html" data-scroll="section-56">Classification Tree</a></li>
<li><a href="section-57.html" data-scroll="section-57">High Dimensional Data and Decision Trees</a></li>
<li><a href="section-58.html" data-scroll="section-58">Discussion of Decision Tree Algorithms</a></li>
</ul>
</li>
<li class="link">
<a href="neural-networks.html" data-scroll="neural-networks"><span class="codenumber">15</span> <span class="title">Neural Networks</span></a><ul>
<li><a href="section-59.html" data-scroll="section-59">Neural Network for Regression</a></li>
<li><a href="section-60.html" data-scroll="section-60">Neural Networks for Classification</a></li>
<li><a href="section-61.html" data-scroll="section-61">Neural Network with a Large Number of Features</a></li>
</ul>
</li>
<li class="link">
<a href="ensemble.html" data-scroll="ensemble"><span class="codenumber">16</span> <span class="title">General Ensemble Models</span></a><ul>
<li><a href="section-62.html" data-scroll="section-62">Why do Ensemble Models Work</a></li>
<li><a href="section-63.html" data-scroll="section-63">Ensemble Models for Regression</a></li>
</ul>
</li>
<li class="link">
<a href="unsupervised.html" data-scroll="unsupervised"><span class="codenumber">17</span> <span class="title">Unsupervised Learning</span></a><ul><li><a href="section-64.html" data-scroll="section-64">Clustering</a></li></ul>
</li>
</ul></nav><div class="extras"><nav><a class="mathbook-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content"><section class="section" id="section-36"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">6.3</span> <span class="title">Dealing with Categorical Data</span>
</h2>
<section class="introduction" id="introduction-18"><p id="p-530">There are couple of techniques for working with categorical data you should be aware of. The major issue is that the algorithms in scikitlearn have been designed to work on categorical data represented as integers. This makes sense because the authors of the algorithms do not know what your categories are.</p></section><section class="subsection" id="subsection-57"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">6.3.1</span> <span class="title">Binary Data</span>
</h3>
<p id="p-531">The simplest case is a categorical variable that takes on only two values. These we can safetely code as a \(0\) or \(1\text{;}\) or \(-1\) or \(1\) if that makes more sense for the problem.</p>
<figure class="figure-like" id="listing-148"><pre class="console"><b># Consider the following dataset about homes that sold in a city in Iowa

hd = pa.read_csv('Data Sets/house-prices/train.csv')

hd.head()
</b>   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \
0   1          60       RL         65.0     8450   Pave   NaN      Reg   
1   2          20       RL         80.0     9600   Pave   NaN      Reg   
2   3          60       RL         68.0    11250   Pave   NaN      IR1   
3   4          70       RL         60.0     9550   Pave   NaN      IR1   
4   5          60       RL         84.0    14260   Pave   NaN      IR1   

  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \
0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   
1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   
2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   
3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   
4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   

  YrSold  SaleType  SaleCondition  SalePrice  
0   2008        WD         Normal     208500  
1   2007        WD         Normal     181500  
2   2008        WD         Normal     223500  
3   2006        WD        Abnorml     140000  
4   2008        WD         Normal     250000  

[5 rows x 81 columns]
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">6.3.1.</span> </figcaption></figure><figure class="figure-like" id="listing-149"><pre class="console"><b># For example the Street feature takes only two values
set(hd.Street)
</b>{'Grvl', 'Pave'}
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">6.3.2.</span> </figcaption></figure><figure class="figure-like" id="listing-150"><pre class="console"><b># We can recode it using a dictionary and .map()
Street_dict = {'Grvl':0, 'Pave':1, 0:0, 1:1}  
# Note we include the trivial coding of the new values as otherwise if we run this twice
# it produces NaN values for the Street feature.
hd.Street = hd.Street.map(Street_dict)
set(hd.Street)
</b>{0, 1}
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">6.3.3.</span> </figcaption></figure></section><section class="subsection" id="subsection-58"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">6.3.2</span> <span class="title">Non Binary Data</span>
</h3>
<p id="p-532">Categorical data could be ordered or unordered. It is tempting to to take ordered data and code it as 0, 1, 2, <ol id="p-533" class="decimal"><li id="li-148"><p id="p-534">However we need ot be careful. Consider the following examples:</p></li></ol></p>
<figure class="figure-like" id="listing-151"><pre class="console"><b>set(hd.Alley)
</b>{'Grvl', 'Pave', nan}
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">6.3.4.</span> </figcaption></figure><p id="p-535">nan in this case almost certainly means that the house does not have an Alley. Note that we could recode this feature as: 'Pave' = 1; 'Grvl'=<ol id="p-536" class="decimal"><li id="li-149"><p id="p-537">5; and nan=0.</p></li></ol></p>
<p id="p-538">However this implies that the difference between No Alley and a Gravel Alley is equal to the difference beetween a Gravel Alley and a Paved Alley, and we really have no way of judging that.</p>
<p id="p-539">The way to deal with these non-binary cases is to use what is called <em class="emphasis">One Hot Encoding</em>. We create three new features:  Alley_Grvl, Alley_Pave, and Alley_nan and then code them each as a 1 if the Alley feature has that value and a 0 otherwise. Here is a code snippet One Hot Encoding a feature.</p>
<figure class="figure-like" id="listing-152"><pre class="console"><b>def onehot(df, feature):
    '''A function to do one-hot-encoding of a feature from a dataframe. df = dataframe'''

    v = list(set(df[feature])) # Make an iterable of the unique values for the feature
    
    for c in df.index: # cycle through the samples
        t = df.loc[c, feature]
        
        for test in v:
            if pa.isna(test):  # nan values are sort of a problem and have to be handled separately
                if pa.isna(t):
                    df.loc[c, '{}_nan'.format(feature)] = 1
                else:
                    df.loc[c, '{}_nan'.format(feature)] = 0
            else:
                if t == test:
                    df.loc[c, '{}_{}'.format(feature, test)] = 1  # Makes a new feature with name feature_value
                                                              # and codes it as a 1 if that was the value
                else:
                    df.loc[c, '{}_{}'.format(feature, test)] = 0  # and 0 otherwise
            
    return df.drop(feature, axis=1) # returns a dataframe with the encoded feature removed
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">6.3.5.</span> </figcaption></figure><figure class="figure-like" id="listing-153"><pre class="console"><b>hd2 = onehot(hd, 'Alley')
hd2[30:40]
</b>    Id  MSSubClass MSZoning  LotFrontage  LotArea  Street LotShape  \
30  31          70  C (all)         50.0     8500       1      Reg   
31  32          20       RL          NaN     8544       1      IR1   
32  33          20       RL         85.0    11049       1      Reg   
33  34          20       RL         70.0    10552       1      IR1   
34  35         120       RL         60.0     7313       1      Reg   
35  36          60       RL        108.0    13418       1      Reg   
36  37          20       RL        112.0    10859       1      Reg   
37  38          20       RL         74.0     8532       1      Reg   
38  39          20       RL         68.0     7922       1      Reg   
39  40          90       RL         65.0     6040       1      Reg   

   LandContour Utilities LotConfig  ... MiscFeature MiscVal MoSold YrSold  \
30         Lvl    AllPub    Inside  ...         NaN       0      7   2008   
31         Lvl    AllPub   CulDSac  ...         NaN       0      6   2008   
32         Lvl    AllPub    Corner  ...         NaN       0      1   2008   
33         Lvl    AllPub    Inside  ...         NaN       0      4   2010   
34         Lvl    AllPub    Inside  ...         NaN       0      8   2007   
35         Lvl    AllPub    Inside  ...         NaN       0      9   2006   
36         Lvl    AllPub    Corner  ...         NaN       0      6   2009   
37         Lvl    AllPub    Inside  ...         NaN       0     10   2009   
38         Lvl    AllPub    Inside  ...         NaN       0      1   2010   
39         Lvl    AllPub    Inside  ...         NaN       0      6   2008   

   SaleType SaleCondition  SalePrice  Alley_nan  Alley_Pave  Alley_Grvl  
30       WD        Normal      40000        0.0         1.0         0.0  
31       WD        Normal     149350        1.0         0.0         0.0  
32       WD        Normal     179900        1.0         0.0         0.0  
33       WD        Normal     165500        1.0         0.0         0.0  
34       WD        Normal     277500        1.0         0.0         0.0  
35       WD        Normal     309000        1.0         0.0         0.0  
36       WD        Normal     145000        1.0         0.0         0.0  
37       WD        Normal     153000        1.0         0.0         0.0  
38       WD       Abnorml     109000        1.0         0.0         0.0  
39       WD       AdjLand      82000        1.0         0.0         0.0  

[10 rows x 83 columns]
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">6.3.6.</span> </figcaption></figure><figure class="figure-like" id="listing-154"><pre class="console"><b>set(hd.ExterQual)
</b>{'Ex', 'Fa', 'Gd', 'TA'}
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">6.3.7.</span> </figcaption></figure><p id="p-540">For the Exterior Quality feature (and the other Quality features) there is a clear order to the values, but again what is not clear is if the distance between <em class="emphasis">Excellent</em> and <em class="emphasis">Good</em> is the same as the distance between <em class="emphasis">Good</em> and <em class="emphasis">Fair</em>. It is maybe not a bad assumption for these features, but might be one worth checking.</p>
<p id="p-541"><em class="alert">What do I mean by Checking?</em> Develop a model using each encoding method and see what the effect on the errors is.</p>
<figure class="figure-like" id="listing-155"><pre class="console"><b>hd3 = onehot(hd2, 'ExterQual')
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">6.3.8.</span> </figcaption></figure><p id="p-542">So notice what this means now: We can now produce a matrix with numerical entries that correspond to the categorical variables. So just with these two, plus the existing numerical features we have.</p>
<figure class="figure-like" id="listing-156"><pre class="console"><b>keep = ['LotArea', '1stFlrSF', '2ndFlrSF', 'BedroomAbvGr', 'FullBath', 'HalfBath', 'SalePrice', 
      'Alley_nan', 'Alley_Pave', 'Alley_Grvl', 'ExterQual_Fa', 'ExterQual_Gd', 'ExterQual_TA', 'ExterQual_Ex']
hd4 = hd3.loc[:, keep]
hd4.head()
</b>   LotArea  1stFlrSF  2ndFlrSF  BedroomAbvGr  FullBath  HalfBath  SalePrice  \
0     8450       856       854             3         2         1     208500   
1     9600      1262         0             3         2         0     181500   
2    11250       920       866             3         2         1     223500   
3     9550       961       756             3         1         0     140000   
4    14260      1145      1053             4         2         1     250000   

   Alley_nan  Alley_Pave  Alley_Grvl  ExterQual_Fa  ExterQual_Gd  \
0        1.0         0.0         0.0           0.0           1.0   
1        1.0         0.0         0.0           0.0           0.0   
2        1.0         0.0         0.0           0.0           1.0   
3        1.0         0.0         0.0           0.0           0.0   
4        1.0         0.0         0.0           0.0           1.0   

   ExterQual_TA  ExterQual_Ex  
0           0.0           0.0  
1           1.0           0.0  
2           0.0           0.0  
3           1.0           0.0  
4           0.0           0.0
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">6.3.9.</span> </figcaption></figure><figure class="figure-like" id="listing-157"><pre class="console"><b># Convert them to Numpy Arrays X for predictors and y for result

keep.remove('SalePrice')
X = np.array(hd4.loc[:, keep])
y = np.array(hd4.loc[:, 'SalePrice'])
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">6.3.10.</span> </figcaption></figure><figure class="figure-like" id="listing-158"><pre class="console"><b>import numpy.random as rn
from sklearn.linear_model import LinearRegression
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">6.3.11.</span> </figcaption></figure><figure class="figure-like" id="listing-159"><pre class="console"><b># We shuffle the data using a random permutation

n = X.shape[0]
test = int(0.20*n)            # We will use a test set made up of 20% of the data from our sample
perm = rn.permutation(n)   
X = X[perm]
y = y[perm]
X_test = X[:test]       # Then create the test
y_test = y[:test]
X_train = X[test:]     # and train sets
y_train = y[test:]
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">6.3.12.</span> </figcaption></figure><figure class="figure-like" id="listing-160"><pre class="console"><b>reg = LinearRegression().fit(X_train, y_train)
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">6.3.13.</span> </figcaption></figure><figure class="figure-like" id="listing-161"><pre class="console"><b>print('Training R2: {}'.format(reg.score(X_train, y_train)))
print('Testing R2: {}'.format(reg.score(X_test, y_test)))
</b>Training R2: 0.7146735662901889
Testing R2: 0.7815549108505843
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">6.3.14.</span> </figcaption></figure></section><section class="subsection" id="subsection-59"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">6.3.3</span> <span class="title">Including Additional Features Gives a Better Prediction</span>
</h3>
<p id="p-543">So rarely does this work as well as it did for me here, but sometimes (<em class="alert">not always</em>) including additional features improves our model. In this case we get a significant improvement in the \(R^2\) value.</p>
<figure class="figure-like" id="listing-162"><pre class="console"><b># What if we did not use the categorical variables we included and just the numerical ones

reg2 = LinearRegression().fit(X_train[:, 0:6], y_train)
print('Training R2: {}'.format(reg2.score(X_train[:, 0:6], y_train)))
print('Testing R2: {}'.format(reg2.score(X_test[:, 0:6], y_test)))
</b>Training R2: 0.6299674886272887
Testing R2: 0.676749202792158
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">6.3.15.</span> </figcaption></figure><p id="p-544">That said. How important are the categorical features here in the model. We can check that by checking the value of their coeeficients:</p>
<figure class="figure-like" id="listing-163"><pre class="console"><b>coef = np.concatenate( (reg.coef_.reshape(1, 13), np.concatenate( (reg2.coef_, [0]*7) ).reshape(1, 13) ), 
               axis = 0)
coef_df = pa.DataFrame(coef, index = ['reg', 'reg2'], columns = keep)
coef_df
# I use a pandas.DataFrame here because it lets me line up the names of the features with the values
</b>       LotArea    1stFlrSF   2ndFlrSF  BedroomAbvGr      FullBath  \
reg   0.710702   92.767874  54.369591  -8343.869009  13515.032172   
reg2  0.451522  122.592604  62.175946 -20240.845648  34097.817634   

          HalfBath     Alley_nan   Alley_Pave   Alley_Grvl  ExterQual_Fa  \
reg   11782.730563  11127.877444 -1272.437752 -9855.439693 -67365.331855   
reg2  23364.091628      0.000000     0.000000     0.000000      0.000000   

      ExterQual_Gd  ExterQual_TA  ExterQual_Ex  
reg   14352.873781  -32433.70115  85446.159224  
reg2      0.000000       0.00000      0.000000
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">6.3.16.</span> </figcaption></figure><p id="p-545">The parameters for the model with categorical variables are significant. Note that the negative values mean that these features subtract from the value and the positive values add to them.</p>
<p id="p-546">For example Exterior Quality <em class="emphasis">Typical/Average</em> is a negative; and Exterior Quality <em class="emphasis">Fair</em> is a negative.</p>
<p id="p-547">But notice the interesting thing we learn:  <em class="emphasis">Alley Missing</em> is a positive, and <em class="emphasis">Alley Paved</em> and <em class="emphasis">Alley Gravel</em> are both negatives. This means our naive idea of coding <em class="emphasis">Alley</em> as 0, <ol id="p-548" class="decimal"><li id="li-150"><p id="p-549">5, 1.0 would have completely led our model astray!</p></li></ol></p></section></section></div></main>
</div>
<div class="login-link"><span id="loginlogout" class="login">login</span></div>
<script src="https://pretextbook.org/js/0.12/login.js"></script>
</body>
</html>
