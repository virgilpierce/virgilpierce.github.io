<!DOCTYPE html>
<!--**************************************-->
<!--*    Generated from PreTeXt source   *-->
<!--*    on 2019-09-17T10:59:20-06:00    *-->
<!--*                                    *-->
<!--*      https://pretextbook.org       *-->
<!--*                                    *-->
<!--**************************************-->
<html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Error in Categorization Problems</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width,  initial-scale=1.0, user-scalable=0, minimum-scale=1.0, maximum-scale=1.0">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['\\(','\\)']]
    },
    asciimath2jax: {
        ignoreClass: ".*",
        processClass: "has_am"
    },
    jax: ["input/AsciiMath"],
    extensions: ["asciimath2jax.js"],
    TeX: {
        extensions: ["extpfeil.js", "autobold.js", "https://pretextbook.org/js/lib/mathjaxknowl.js", ],
        // scrolling to fragment identifiers is controlled by other Javascript
        positionToHash: false,
        equationNumbers: { autoNumber: "none", useLabelIds: true, },
        TagSide: "right",
        TagIndent: ".8em",
    },
    // HTML-CSS output Jax to be dropped for MathJax 3.0
    "HTML-CSS": {
        scale: 88,
        mtextFontInherit: true,
    },
    CommonHTML: {
        scale: 88,
        mtextFontInherit: true,
    },
});
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML-full"></script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.12/pretext.js"></script><script src="https://pretextbook.org/js/0.12/pretext_add_on.js"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/toc.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/colors_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/setcolors.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/features.css" rel="stylesheet" type="text/css">
<script>var logged_in = false;
var role = 'student';
var guest_access = true;
var login_required = false;
var js_version = 0.12;
</script>
</head>
<body class="mathbook-book has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div class="hidden-content" style="display:none">\(\newcommand{\doubler}[1]{2#1}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\)</div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="datascience.html"><span class="title">Data Science with Python</span></a></h1>
<p class="byline">Virgil U Pierce</p>
</div>
</div></div>
<nav id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3"><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="section-32.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="process.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="wrangling.html" title="Next">Next</a></span></div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="section-32.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="process.html" title="Up">Up</a><a class="next-button button toolbar-item" href="wrangling.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link"><a href="frontmatter-1.html" data-scroll="frontmatter-1"><span class="title">Front Matter</span></a></li>
<li class="link">
<a href="course_syllabus.html" data-scroll="course_syllabus"><span class="codenumber">1</span> <span class="title">Course Syllabus</span></a><ul>
<li><a href="section-1.html" data-scroll="section-1">Class and Instructor Details</a></li>
<li><a href="section-2.html" data-scroll="section-2">Course Description</a></li>
<li><a href="section-3.html" data-scroll="section-3">Textbook and Software</a></li>
<li><a href="section-4.html" data-scroll="section-4">Learning Objectives / Outcomes for the Course</a></li>
<li><a href="section-5.html" data-scroll="section-5">Communicating</a></li>
<li><a href="section-6.html" data-scroll="section-6">Course Outline</a></li>
<li><a href="section-7.html" data-scroll="section-7">Connections with Industrial Mathematics</a></li>
<li><a href="section-8.html" data-scroll="section-8">Assessments</a></li>
<li><a href="section-9.html" data-scroll="section-9">UNCO Policy Statements</a></li>
</ul>
</li>
<li class="link">
<a href="data_science_introduction.html" data-scroll="data_science_introduction"><span class="codenumber">2</span> <span class="title">Introduction to Data Science</span></a><ul>
<li><a href="section-10.html" data-scroll="section-10">Anaconda, Jupyter, and Python</a></li>
<li><a href="section-11.html" data-scroll="section-11">Github</a></li>
<li><a href="section-12.html" data-scroll="section-12">Python</a></li>
<li><a href="section-13.html" data-scroll="section-13">Some Prelimaries</a></li>
<li><a href="section-14.html" data-scroll="section-14">First Motivating Example - Baseball Players</a></li>
<li><a href="section-15.html" data-scroll="section-15">Second Motivating Example - Abalone Characteristics</a></li>
<li><a href="section-16.html" data-scroll="section-16">Third Motivating Example - US Income Levels</a></li>
<li><a href="section-17.html" data-scroll="section-17">Fourth Motivating Example - Mushroom Characteristics</a></li>
<li><a href="section-18.html" data-scroll="section-18">Fifth Motivating Example - House Prices</a></li>
<li><a href="section-19.html" data-scroll="section-19">Sixth Motivating Example - Running Data from Garmin</a></li>
<li><a href="section-20.html" data-scroll="section-20">Seventh Motivating Example - Berlin Airbnb Data</a></li>
<li><a href="section-21.html" data-scroll="section-21">Eighth Motivating Example - Colorado Child Care</a></li>
<li><a href="section-22.html" data-scroll="section-22">Ninth Motivating Example - Flight Delays at DEN</a></li>
<li><a href="section-23.html" data-scroll="section-23">Tenth Motivating Example - Image Classification</a></li>
<li><a href="section-24.html" data-scroll="section-24">Left for a future class - Unsupervised Learning</a></li>
</ul>
</li>
<li class="link">
<a href="data.html" data-scroll="data"><span class="codenumber">3</span> <span class="title">Data</span></a><ul>
<li><a href="section-25.html" data-scroll="section-25">What is Data</a></li>
<li><a href="section-26.html" data-scroll="section-26">Supervised versus Unsupervised Learning</a></li>
<li><a href="section-27.html" data-scroll="section-27">Where to get Data</a></li>
</ul>
</li>
<li class="link">
<a href="tools.html" data-scroll="tools"><span class="codenumber">4</span> <span class="title">Tools of the Trade</span></a><ul>
<li><a href="section-28.html" data-scroll="section-28">Python and Jupyter</a></li>
<li><a href="section-29.html" data-scroll="section-29">Development</a></li>
<li><a href="section-30.html" data-scroll="section-30">Versioning Control</a></li>
</ul>
</li>
<li class="link">
<a href="process.html" data-scroll="process"><span class="codenumber">5</span> <span class="title">The Data Science Process</span></a><ul>
<li><a href="section-31.html" data-scroll="section-31">Professional Ethics</a></li>
<li><a href="section-32.html" data-scroll="section-32">Controlling for Error</a></li>
<li><a href="section-33.html" data-scroll="section-33" class="active">Error in Categorization Problems</a></li>
</ul>
</li>
<li class="link">
<a href="wrangling.html" data-scroll="wrangling"><span class="codenumber">6</span> <span class="title">Wrangling the Data</span></a><ul>
<li><a href="section-34.html" data-scroll="section-34">Formatting the Data</a></li>
<li><a href="section-35.html" data-scroll="section-35">Dealing with Strings</a></li>
<li><a href="section-36.html" data-scroll="section-36">Dealing with Categorical Data</a></li>
<li><a href="section-37.html" data-scroll="section-37">Dealing with Missing Data</a></li>
<li><a href="section-38.html" data-scroll="section-38">Dealing with Images</a></li>
</ul>
</li>
<li class="link">
<a href="resampling.html" data-scroll="resampling"><span class="codenumber">7</span> <span class="title">Resampling</span></a><ul>
<li><a href="section-39.html" data-scroll="section-39">Cross Validation</a></li>
<li><a href="section-40.html" data-scroll="section-40">Bootstraps</a></li>
</ul>
</li>
<li class="link">
<a href="EDA.html" data-scroll="EDA"><span class="codenumber">8</span> <span class="title">Exploratory Data Analysis</span></a><ul><li><a href="section-41.html" data-scroll="section-41">Nonlinear Relations</a></li></ul>
</li>
<li class="link">
<a href="linear_regression.html" data-scroll="linear_regression"><span class="codenumber">9</span> <span class="title">Linear Regression</span></a><ul>
<li><a href="section-42.html" data-scroll="section-42">Calculus Approach to Linear Regression</a></li>
<li><a href="section-43.html" data-scroll="section-43">Linear Regression as Projection</a></li>
</ul>
</li>
<li class="link">
<a href="pca.html" data-scroll="pca"><span class="codenumber">10</span> <span class="title">Principal Component Analysis</span></a><ul>
<li><a href="section-44.html" data-scroll="section-44">Eigenvalue Decomposition of Square Matrix</a></li>
<li><a href="section-45.html" data-scroll="section-45">Singular Value Decomposition</a></li>
</ul>
</li>
</ul></nav><div class="extras"><nav><a class="mathbook-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content"><section class="section" id="section-33"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">5.3</span> <span class="title">Error in Categorization Problems</span>
</h2>
<section class="introduction" id="introduction-16"><p id="p-488">Consider a categorization problem, this dataset is a collection of data related to study of chronic kidney disease (from the UCI archive).</p>
<figure class="figure-like" id="listing-110"><pre class="console"><b>kd = pa.read_csv('Data Sets/Kidney/chronic_kidney_disease.csv', 
                 names=['age', 'bp', 'specific_gravity', 'albumin', 'sugar', 'red_blood_cells', 'pus_cell',
                        'pus_cell_clumps', 'bacteria', 'blood_glucose', 'blood_urea', 'serum_creatinine', 
                        'sodium', 'potassium', 'hemoglobin', 'packed_cell_volume', 'white_blood_cell_count',
                        'red_blood_cell_count', 'hypertension', 'diabetes_mellitus', 'coronary_artery_disease',
                        'appetite', 'pedal_edema', 'anemia', 'class'])

kd.loc[kd.loc[:, 'class'] == 'ckd\t', 'class'] = 'ckd'  # There is an extra tab character on some of the ckd values

kd.head()
</b>  age  bp specific_gravity albumin sugar red_blood_cells  pus_cell  \
0  48  80            1.020       1     0               ?    normal   
1   7  50            1.020       4     0               ?    normal   
2  62  80            1.010       2     3          normal    normal   
3  48  70            1.005       4     0          normal  abnormal   
4  51  80            1.010       2     0          normal    normal   

  pus_cell_clumps    bacteria blood_glucose  ... packed_cell_volume  \
0      notpresent  notpresent           121  ...                 44   
1      notpresent  notpresent             ?  ...                 38   
2      notpresent  notpresent           423  ...                 31   
3         present  notpresent           117  ...                 32   
4      notpresent  notpresent           106  ...                 35   

  white_blood_cell_count red_blood_cell_count hypertension diabetes_mellitus  \
0                   7800                  5.2          yes               yes   
1                   6000                    ?           no                no   
2                   7500                    ?           no               yes   
3                   6700                  3.9          yes                no   
4                   7300                  4.6           no                no   

  coronary_artery_disease appetite pedal_edema anemia class  
0                      no     good          no     no   ckd  
1                      no     good          no     no   ckd  
2                      no     poor          no    yes   ckd  
3                      no     poor         yes    yes   ckd  
4                      no     good          no     no   ckd  

[5 rows x 25 columns]
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">5.3.1.</span> </figcaption></figure><figure class="figure-like" id="listing-111"><pre class="console"><b># There are some missing values for hemoglobin

kd = kd[kd.hemoglobin != '?']
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">5.3.2.</span> </figcaption></figure><p id="p-489">Note that the goal here is to determine a hemoglobin lRSSel that indicates that a patient has <em class="emphasis">Chronic Kidney Disease</em>.</p></section><section class="subsection" id="subsection-56"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">5.3.1</span> <span class="title">Logistic Regression (Classification)</span>
</h3>
<p id="p-490">We first need to dRSSelop a model for classification to discuss how error in categorization models is handled. Our first example of a categorization model is a generalization of the <em class="emphasis">Linear Regression</em> model we were using above.</p>
<p id="p-491">We will consider two models. The first is based on the linear interpolation we used above, and is what we might consider a <em class="emphasis">naive</em> generalization. The second is <em class="emphasis">Logistic Regression</em>, which despite its name, is derived first as a classification method.</p>
<p id="p-492">In order to compare our two methods, we will use the <em class="emphasis">test</em> set method we introduced above and set aside a portion of our samples to use to train or dRSSelop the models and a separate portion to use for testing the models.</p>
<figure class="figure-like" id="listing-112"><pre class="console"><b># First convert the data to numerical data in numpy arrays
# Note a problem with this data is that the float values for hemoglobin (and other variables) were read as strings
# We need to convert them

X = np.atleast_2d(np.array([float(h) for h in kd.loc[:, 'hemoglobin']])).transpose()

# We need to recode the values for the class to be numeric
rename_class = { 'ckd':1, 'notckd':0}  
y = np.array([rename_class[c] for c in np.array(kd.loc[:, 'class'])])
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">5.3.3.</span> </figcaption></figure><figure class="figure-like" id="listing-113"><pre class="console"><b>plt.figure(figsize = (8, 6))
plt.plot(X, y, 'b.');
</b></pre>
<div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="Images/5.5.png" style="width: 100%; height: auto;" alt=""></div>
<figcaption><span class="type">Listing</span> <span class="codenumber">5.3.4.</span> </figcaption></figure><p id="p-493">Note that other than the fact that the \(y\) values take only 0 or 1 values, this is a similar problem to the regression problem above. So we could try Linear Regression. First we divide the data into testing and training sets.</p>
<figure class="figure-like" id="listing-114"><pre class="console"><b># We shuffle the data using a random permutation

n = X.shape[0]
test = int(0.20*n)            # We will use a test set made up of 20% of the data from our sample
perm = rn.permutation(n)   
X = X[perm]
y = y[perm]
X_test = X[:test]       # Then create the test
y_test = y[:test]
X_train = X[test:]     # and train sets
y_train = y[test:]
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">5.3.5.</span> </figcaption></figure><figure class="figure-like" id="listing-115"><pre class="console"><b>reg = LinearRegression().fit(X_train, y_train)

b, m = reg.intercept_, *reg.coef_
b, m
</b>(2.2345537069285486, -0.13212090094166123)
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">5.3.6.</span> </figcaption></figure><figure class="figure-like" id="listing-116"><pre class="console"><b># we round the resulting y values to get the nearest value of 1 or 0
# we round the resulting y values to get the nearest value of 1 or 0

xhat = np.linspace(2, 18, 100)
yhat = m*xhat+b
plt.figure(figsize = (8, 6))
plt.plot(X_train, y_train, 'b.')
plt.plot(xhat, yhat, 'r-');
</b></pre>
<div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="Images/5.6.png" style="width: 100%; height: auto;" alt=""></div>
<figcaption><span class="type">Listing</span> <span class="codenumber">5.3.7.</span> </figcaption></figure><figure class="figure-like" id="listing-117"><pre class="console"><b># we round the resulting y values to get the nearest value of 1 or 0

xhat = np.linspace(2, 18, 100)
fix_yhat = {2:1, 1:1, 0:0}
yhat = [fix_yhat[h] for h in np.round(m*xhat+b)]
plt.figure(figsize = (8, 6))
plt.plot(X_train, y_train, 'b.')
plt.plot(xhat, yhat, 'r-');
</b></pre>
<div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="Images/5.7.png" style="width: 100%; height: auto;" alt=""></div>
<figcaption><span class="type">Listing</span> <span class="codenumber">5.3.8.</span> </figcaption></figure><p id="p-494">The decision boundary is where \(y =  mx + b = 0.5\text{.}\)</p>
<figure class="figure-like" id="listing-118"><pre class="console"><b>dec_boundary = (0.5 - b)/m
dec_boundary
</b>13.128533748755249
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">5.3.9.</span> </figcaption></figure><p id="p-495">The points to the left of the decision boundary can be classified as Chronic Kidney Disease, those to the right as healthy. HowRSSer note that there will be some samples that are misclassified. The proportion of the sample (training and testing separately) that is misclassified is the error.</p>
<figure class="figure-like" id="listing-119"><pre class="console"><b>plt.figure(figsize = (8, 6))

plt.plot(X_train[(X_train &lt; dec_boundary)[:, 0]], y_train[(X_train &lt; dec_boundary)[:, 0]], 'b.');
plt.plot(X_train[(X_train &gt; dec_boundary)[:, 0]], y_train[(X_train &gt; dec_boundary)[:, 0]], 'r.');
plt.plot(X_test[(X_test &lt; dec_boundary)[:, 0]], y_test[(X_test &lt; dec_boundary)[:, 0]], 'b+');
plt.plot(X_test[(X_test &gt; dec_boundary)[:, 0]], y_test[(X_test &gt; dec_boundary)[:, 0]], 'r+');
</b></pre>
<div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="Images/5.8.png" style="width: 100%; height: auto;" alt=""></div>
<figcaption><span class="type">Listing</span> <span class="codenumber">5.3.10.</span> </figcaption></figure><figure class="figure-like" id="listing-120"><pre class="console"><b>Error_train = (sum( y_train[(X_train &lt; dec_boundary)[:, 0]] == 0 ) + 
               sum( y_train[(X_train &gt; dec_boundary)[:, 0]] == 1) )/y_train.shape[0]
Error_test = (sum( y_test[(X_test &lt; dec_boundary)[:, 0]] == 0 ) + 
               sum( y_test[(X_test &gt; dec_boundary)[:, 0]] == 1) )/y_test.shape[0]

print('Training Error: {}'.format(Error_train))
print('Testing Error: {}'.format(Error_test))
</b>Training Error: 0.06093189964157706
Testing Error: 0.10144927536231885
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">5.3.11.</span> </figcaption></figure><article class="paragraphs" id="paragraphs-17"><h5 class="heading"><span class="title">The Logistic Model.</span></h5>
<p id="p-496">Note though that what we have done does not really make a lot of sense. We have used a linear best fit line to try and predict the values of the categorical variable \(y\text{.}\) In this case \(y\) only takes values \(1\) or \(0\) and so it is not to far off, howRSSer if we had a categorical variable with 3 or more values coding them as integers may not correspond to their relationship. For one thing they might not RSSen be ordered!</p>
<p id="p-497">What we need is a method of categorization that is built on the categorical problem itself.</p>
<p id="p-498">What we are trying to predict is the probability that a patient has <em class="emphasis">Chronic Kidney Disease</em> given a hemoglobin lRSSel \(x\text{.}\)</p>
<div class="displaymath" id="p-499">
\begin{equation*}
p(x) = P(y=1 | x)
\end{equation*}
</div>
<p id="p-500">Note that with <em class="emphasis">Linear Regression</em> what we have attempted to do is estimate this probability with a linear function:</p>
<div class="displaymath" id="p-501">
\begin{equation*}
p(x) = P(y=1 | x) = m x + b
\end{equation*}
</div>
<p id="p-502">But immediately notice the problem, this returns values bigger than 1 and less than 0 for some choices of \(x\text{.}\)</p>
<p id="p-503">So what we would prefer is to estimate \(p(x) = P(y=1 | x) \) with some function that only returns values between 0 and 1 and thus gives a valid probability. We could use the logistic function:</p>
<div class="displaymath" id="p-504">
\begin{equation*}
p(x) = P(y=1 | x) = \frac{\exp(m x + b)}{1 + \exp(m x + b)}
\end{equation*}
</div>
<p id="p-505">You may have seen this function before in modeling population growth with a carrying capacity.</p>
<p id="p-506">So the idea is to find values of \(m\) and \(b\) that give the best estimate for \(p(x)\) that we can find.</p>
<p id="p-507">We will skip for now how these values are actually computed and instead use the scikitlearn function LogisticRegression.</p>
<figure class="figure-like" id="listing-121"><pre class="console"><b># Import LogisticRegression
from sklearn.linear_model import LogisticRegression
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">5.3.12.</span> </figcaption></figure><figure class="figure-like" id="listing-122"><pre class="console"><b># Make an instance of the model and fit it to the training data
lgreg = LogisticRegression(solver='lbfgs')
lgreg.fit(X_train, y_train);
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">5.3.13.</span> </figcaption></figure><figure class="figure-like" id="listing-123"><pre class="console"><b>b, m = *lgreg.intercept_, *lgreg.coef_[0]
b, m
</b>(21.633631288846512, -1.613513243575066)
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">5.3.14.</span> </figcaption></figure><figure class="figure-like" id="listing-124"><pre class="console"><b>plt.figure(figsize = (8, 6))
plt.plot(X_train, y_train, 'b.')
xx = np.linspace(0, 20, 100)
yy = np.exp(m*xx+b)/(1 + np.exp(m*xx+b))
plt.plot(xx, yy, 'g-');
</b></pre>
<div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="Images/5.9.png" style="width: 100%; height: auto;" alt=""></div>
<figcaption><span class="type">Listing</span> <span class="codenumber">5.3.15.</span> </figcaption></figure><p id="p-508">The decsision boundary happens again where the probability becomes &gt;0.5; although note that we could actually tune the location of the decsision boundary based on the extent to which we want to avoid false positives or false negatives.</p>
<figure class="figure-like" id="listing-125"><pre class="console"><b>dec_bd = -b/m  # Solving for the logistic function = 0.5 gives - b/m
dec_bd
</b>13.407780428819295
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">5.3.16.</span> </figcaption></figure><figure class="figure-like" id="listing-126"><pre class="console"><b># Plotting the classification of the training and testing sets.

plt.figure(figsize = (8, 6))

plt.plot(X_train[(X_train &lt; dec_bd)[:, 0]], y_train[(X_train &lt; dec_bd)[:, 0]], 'b.')
plt.plot(X_train[(X_train &gt; dec_bd)[:, 0]], y_train[(X_train &gt; dec_bd)[:, 0]], 'r.')
plt.plot(X_test[(X_test &lt; dec_bd)[:, 0]], y_test[(X_test &lt; dec_bd)[:, 0]], 'b+')
plt.plot(X_test[(X_test &gt; dec_bd)[:, 0]], y_test[(X_test &gt; dec_bd)[:, 0]], 'r+')
plt.plot(xx, yy, 'g-');
</b></pre>
<div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="Images/5.10.png" style="width: 100%; height: auto;" alt=""></div>
<figcaption><span class="type">Listing</span> <span class="codenumber">5.3.17.</span> </figcaption></figure><figure class="figure-like" id="listing-127"><pre class="console"><b># Computing the error of the training and testing sets.

Error_train = (sum( y_train[(X_train &lt; dec_bd)[:, 0]] == 0 ) + 
               sum( y_train[(X_train &gt; dec_bd)[:, 0]] == 1) )/y_train.shape[0]
Error_test = (sum( y_test[(X_test &lt; dec_bd)[:, 0]] == 0 ) + 
               sum( y_test[(X_test &gt; dec_bd)[:, 0]] == 1) )/y_test.shape[0]

print('Training Error: {}'.format(Error_train))
print('Testing Error: {}'.format(Error_test))
</b>Training Error: 0.07885304659498207
Testing Error: 0.08695652173913043
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">5.3.18.</span> </figcaption></figure><p id="p-509">In this case the error was comparable, and slightly better (although it changes each time you run it).</p>
<p id="p-510">In future weeks we will get into the details of how these models are computed.</p></article></section></section></div></main>
</div>
<div class="login-link"><span id="loginlogout" class="login">login</span></div>
<script src="https://pretextbook.org/js/0.12/login.js"></script>
</body>
</html>
