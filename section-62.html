<!DOCTYPE html>
<!--**************************************-->
<!--*    Generated from PreTeXt source   *-->
<!--*    on 2019-11-19T13:28:13-07:00    *-->
<!--*                                    *-->
<!--*      https://pretextbook.org       *-->
<!--*                                    *-->
<!--**************************************-->
<html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Why do Ensemble Models Work</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width,  initial-scale=1.0, user-scalable=0, minimum-scale=1.0, maximum-scale=1.0">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['\\(','\\)']]
    },
    asciimath2jax: {
        ignoreClass: ".*",
        processClass: "has_am"
    },
    jax: ["input/AsciiMath"],
    extensions: ["asciimath2jax.js"],
    TeX: {
        extensions: ["extpfeil.js", "autobold.js", "https://pretextbook.org/js/lib/mathjaxknowl.js", ],
        // scrolling to fragment identifiers is controlled by other Javascript
        positionToHash: false,
        equationNumbers: { autoNumber: "none", useLabelIds: true, },
        TagSide: "right",
        TagIndent: ".8em",
    },
    // HTML-CSS output Jax to be dropped for MathJax 3.0
    "HTML-CSS": {
        scale: 88,
        mtextFontInherit: true,
    },
    CommonHTML: {
        scale: 88,
        mtextFontInherit: true,
    },
});
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML-full"></script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.12/pretext.js"></script><script src="https://pretextbook.org/js/0.12/pretext_add_on.js"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/toc.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/colors_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/setcolors.css" rel="stylesheet" type="text/css">
<!-- 2019-10-12: Temporary - CSS file for experiments with styling --><link href="developer.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/features.css" rel="stylesheet" type="text/css">
<script>var logged_in = false;
var role = 'student';
var guest_access = true;
var login_required = false;
var js_version = 0.12;
</script>
</head>
<body class="mathbook-book has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div class="hidden-content" style="display:none">\(\newcommand{\doubler}[1]{2#1}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\)</div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="datascience.html"><span class="title">Data Science with Python</span></a></h1>
<p class="byline">Virgil U Pierce</p>
</div>
</div></div>
<nav id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3"><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="ensemble.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="ensemble.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="section-63.html" title="Next">Next</a></span></div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="ensemble.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="ensemble.html" title="Up">Up</a><a class="next-button button toolbar-item" href="section-63.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link"><a href="frontmatter-1.html" data-scroll="frontmatter-1"><span class="title">Front Matter</span></a></li>
<li class="link">
<a href="course_syllabus.html" data-scroll="course_syllabus"><span class="codenumber">1</span> <span class="title">Course Syllabus</span></a><ul>
<li><a href="section-1.html" data-scroll="section-1">Class and Instructor Details</a></li>
<li><a href="section-2.html" data-scroll="section-2">Course Description</a></li>
<li><a href="section-3.html" data-scroll="section-3">Textbook and Software</a></li>
<li><a href="section-4.html" data-scroll="section-4">Learning Objectives / Outcomes for the Course</a></li>
<li><a href="section-5.html" data-scroll="section-5">Communicating</a></li>
<li><a href="section-6.html" data-scroll="section-6">Course Outline</a></li>
<li><a href="section-7.html" data-scroll="section-7">Connections with Industrial Mathematics</a></li>
<li><a href="section-8.html" data-scroll="section-8">Assessments</a></li>
<li><a href="section-9.html" data-scroll="section-9">UNCO Policy Statements</a></li>
</ul>
</li>
<li class="link">
<a href="data_science_introduction.html" data-scroll="data_science_introduction"><span class="codenumber">2</span> <span class="title">Introduction to Data Science</span></a><ul>
<li><a href="section-10.html" data-scroll="section-10">Anaconda, Jupyter, and Python</a></li>
<li><a href="section-11.html" data-scroll="section-11">Github</a></li>
<li><a href="section-12.html" data-scroll="section-12">Python</a></li>
<li><a href="section-13.html" data-scroll="section-13">Some Prelimaries</a></li>
<li><a href="section-14.html" data-scroll="section-14">First Motivating Example - Baseball Players</a></li>
<li><a href="section-15.html" data-scroll="section-15">Second Motivating Example - Abalone Characteristics</a></li>
<li><a href="section-16.html" data-scroll="section-16">Third Motivating Example - US Income Levels</a></li>
<li><a href="section-17.html" data-scroll="section-17">Fourth Motivating Example - Mushroom Characteristics</a></li>
<li><a href="section-18.html" data-scroll="section-18">Fifth Motivating Example - House Prices</a></li>
<li><a href="section-19.html" data-scroll="section-19">Sixth Motivating Example - Running Data from Garmin</a></li>
<li><a href="section-20.html" data-scroll="section-20">Seventh Motivating Example - Berlin Airbnb Data</a></li>
<li><a href="section-21.html" data-scroll="section-21">Eighth Motivating Example - Colorado Child Care</a></li>
<li><a href="section-22.html" data-scroll="section-22">Ninth Motivating Example - Flight Delays at DEN</a></li>
<li><a href="section-23.html" data-scroll="section-23">Tenth Motivating Example - Image Classification</a></li>
<li><a href="section-24.html" data-scroll="section-24">Left for a future class - Unsupervised Learning</a></li>
</ul>
</li>
<li class="link">
<a href="data.html" data-scroll="data"><span class="codenumber">3</span> <span class="title">Data</span></a><ul>
<li><a href="section-25.html" data-scroll="section-25">What is Data</a></li>
<li><a href="section-26.html" data-scroll="section-26">Supervised versus Unsupervised Learning</a></li>
<li><a href="section-27.html" data-scroll="section-27">Where to get Data</a></li>
</ul>
</li>
<li class="link">
<a href="tools.html" data-scroll="tools"><span class="codenumber">4</span> <span class="title">Tools of the Trade</span></a><ul>
<li><a href="section-28.html" data-scroll="section-28">Python and Jupyter</a></li>
<li><a href="section-29.html" data-scroll="section-29">Development</a></li>
<li><a href="section-30.html" data-scroll="section-30">Versioning Control</a></li>
</ul>
</li>
<li class="link">
<a href="process.html" data-scroll="process"><span class="codenumber">5</span> <span class="title">The Data Science Process</span></a><ul>
<li><a href="section-31.html" data-scroll="section-31">Professional Ethics</a></li>
<li><a href="section-32.html" data-scroll="section-32">Controlling for Error</a></li>
<li><a href="section-33.html" data-scroll="section-33">Error in Categorization Problems</a></li>
</ul>
</li>
<li class="link">
<a href="wrangling.html" data-scroll="wrangling"><span class="codenumber">6</span> <span class="title">Wrangling the Data</span></a><ul>
<li><a href="section-34.html" data-scroll="section-34">Formatting the Data</a></li>
<li><a href="section-35.html" data-scroll="section-35">Dealing with Strings</a></li>
<li><a href="section-36.html" data-scroll="section-36">Dealing with Categorical Data</a></li>
<li><a href="section-37.html" data-scroll="section-37">Dealing with Missing Data</a></li>
<li><a href="section-38.html" data-scroll="section-38">Dealing with Images</a></li>
</ul>
</li>
<li class="link">
<a href="resampling.html" data-scroll="resampling"><span class="codenumber">7</span> <span class="title">Resampling</span></a><ul>
<li><a href="section-39.html" data-scroll="section-39">Cross Validation</a></li>
<li><a href="section-40.html" data-scroll="section-40">Bootstraps</a></li>
</ul>
</li>
<li class="link">
<a href="EDA.html" data-scroll="EDA"><span class="codenumber">8</span> <span class="title">Exploratory Data Analysis</span></a><ul><li><a href="section-41.html" data-scroll="section-41">Nonlinear Relations</a></li></ul>
</li>
<li class="link">
<a href="linear_regression.html" data-scroll="linear_regression"><span class="codenumber">9</span> <span class="title">Linear Regression</span></a><ul>
<li><a href="section-42.html" data-scroll="section-42">Calculus Approach to Linear Regression</a></li>
<li><a href="section-43.html" data-scroll="section-43">Linear Regression as Projection</a></li>
</ul>
</li>
<li class="link">
<a href="pca.html" data-scroll="pca"><span class="codenumber">10</span> <span class="title">Principal Component Analysis</span></a><ul>
<li><a href="section-44.html" data-scroll="section-44">Eigenvalue Decomposition of Square Matrix</a></li>
<li><a href="section-45.html" data-scroll="section-45">Singular Value Decomposition</a></li>
</ul>
</li>
<li class="link">
<a href="k-nn.html" data-scroll="k-nn"><span class="codenumber">11</span> <span class="title">k-Nearest Neighbors</span></a><ul>
<li><a href="section-46.html" data-scroll="section-46">Checking Performance with Bootstraps</a></li>
<li><a href="section-47.html" data-scroll="section-47">Normalization</a></li>
<li><a href="section-48.html" data-scroll="section-48">k-Nearest Neighbors with Many Factors</a></li>
<li><a href="section-49.html" data-scroll="section-49">k-Nearest Neighbors for Regression</a></li>
</ul>
</li>
<li class="link">
<a href="ridge_and_lasso.html" data-scroll="ridge_and_lasso"><span class="codenumber">12</span> <span class="title">Ridge and Lasso Regression</span></a><ul>
<li><a href="section-50.html" data-scroll="section-50">House Pricing Data: Linear Regression</a></li>
<li><a href="section-51.html" data-scroll="section-51">Ridge Regression</a></li>
<li><a href="section-52.html" data-scroll="section-52">Lasso Regression</a></li>
</ul>
</li>
<li class="link">
<a href="lda_svm.html" data-scroll="lda_svm"><span class="codenumber">13</span> <span class="title">Linear Discrimant Analysis and Support Vector Machines</span></a><ul>
<li><a href="section-53.html" data-scroll="section-53">Linear Discriminant Analysis</a></li>
<li><a href="section-54.html" data-scroll="section-54">Support Vector Machines</a></li>
</ul>
</li>
<li class="link">
<a href="decision_trees.html" data-scroll="decision_trees"><span class="codenumber">14</span> <span class="title">Decsion Trees</span></a><ul>
<li><a href="section-55.html" data-scroll="section-55">Regression Trees</a></li>
<li><a href="section-56.html" data-scroll="section-56">Classification Tree</a></li>
<li><a href="section-57.html" data-scroll="section-57">High Dimensional Data and Decision Trees</a></li>
<li><a href="section-58.html" data-scroll="section-58">Discussion of Decision Tree Algorithms</a></li>
</ul>
</li>
<li class="link">
<a href="neural-networks.html" data-scroll="neural-networks"><span class="codenumber">15</span> <span class="title">Neural Networks</span></a><ul>
<li><a href="section-59.html" data-scroll="section-59">Neural Network for Regression</a></li>
<li><a href="section-60.html" data-scroll="section-60">Neural Networks for Classification</a></li>
<li><a href="section-61.html" data-scroll="section-61">Neural Network with a Large Number of Features</a></li>
</ul>
</li>
<li class="link">
<a href="ensemble.html" data-scroll="ensemble"><span class="codenumber">16</span> <span class="title">General Ensemble Models</span></a><ul>
<li><a href="section-62.html" data-scroll="section-62" class="active">Why do Ensemble Models Work</a></li>
<li><a href="section-63.html" data-scroll="section-63">Ensemble Models for Regression</a></li>
</ul>
</li>
<li class="link">
<a href="unsupervised.html" data-scroll="unsupervised"><span class="codenumber">17</span> <span class="title">Unsupervised Learning</span></a><ul><li><a href="section-64.html" data-scroll="section-64">Clustering</a></li></ul>
</li>
</ul></nav><div class="extras"><nav><a class="mathbook-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content"><section class="section" id="section-62"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">16.1</span> <span class="title">Why do Ensemble Models Work</span>
</h2>
<section class="introduction" id="introduction-38"><p id="p-901">Today we will look at a method of building ensemble models where the individual components are independently trained models. The key feature for ensemble models is that the individual components are trained on a bootstrapped or random sample of the training data. This introduces some randomness into the ensemble model and helps avoid a full model that has responded to the irreducible error.</p>
<p id="p-902">The method we will look at today is called <em class="emphasis">Voting</em> (as usual there is a Classification and a Regression version of the idea). Consider the kidney disease data set.</p>
<figure class="figure-like" id="listing-510"><pre class="console"><b>import pandas as pa
import matplotlib.pyplot as plt
import matplotlib.colors as pltco
import numpy as np
import numpy.random as rn
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">16.1.1.</span> </figcaption></figure><figure class="figure-like" id="listing-511"><pre class="console"><b>kd = pa.read_csv('Data Sets/Kidney/chronic_kidney_disease.csv', 
                 names=['age', 'bp', 'specific_gravity', 'albumin', 'sugar', 'red_blood_cells', 'pus_cell',
                        'pus_cell_clumps', 'bacteria', 'blood_glucose', 'blood_urea', 'serum_creatinine', 
                        'sodium', 'potassium', 'hemoglobin', 'packed_cell_volume', 'white_blood_cell_count',
                        'red_blood_cell_count', 'hypertension', 'diabetes_mellitus', 'coronary_artery_disease',
                        'appetite', 'pedal_edema', 'anemia', 'class'])

kd.loc[kd.loc[:, 'class'] == 'ckd\t', 'class'] = 'ckd'  # There is an extra tab character on some of the ckd values

kd.head()
</b>  age  bp specific_gravity albumin sugar red_blood_cells  pus_cell  \
0  48  80            1.020       1     0               ?    normal   
1   7  50            1.020       4     0               ?    normal   
2  62  80            1.010       2     3          normal    normal   
3  48  70            1.005       4     0          normal  abnormal   
4  51  80            1.010       2     0          normal    normal   

  pus_cell_clumps    bacteria blood_glucose  ... packed_cell_volume  \
0      notpresent  notpresent           121  ...                 44   
1      notpresent  notpresent             ?  ...                 38   
2      notpresent  notpresent           423  ...                 31   
3         present  notpresent           117  ...                 32   
4      notpresent  notpresent           106  ...                 35   

  white_blood_cell_count red_blood_cell_count hypertension diabetes_mellitus  \
0                   7800                  5.2          yes               yes   
1                   6000                    ?           no                no   
2                   7500                    ?           no               yes   
3                   6700                  3.9          yes                no   
4                   7300                  4.6           no                no   

  coronary_artery_disease appetite pedal_edema anemia class  
0                      no     good          no     no   ckd  
1                      no     good          no     no   ckd  
2                      no     poor          no    yes   ckd  
3                      no     poor         yes    yes   ckd  
4                      no     good          no     no   ckd  

[5 rows x 25 columns]
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">16.1.2.</span> </figcaption></figure><figure class="figure-like" id="listing-512"><pre class="console"><b># There are some missing values for hemoglobin and blood pressure

var1 = 'hemoglobin'
var2 = 'red_blood_cell_count'

kd = kd[kd.loc[:, var1] != '?']
kd = kd[kd.loc[:, var2] != '?']
kd = kd[kd.loc[:, var2] != '\t?']
kd.loc[:, var1] = kd.loc[:, var1].astype('float')
kd.loc[:, var2] = kd.loc[:, var2].astype('float')
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">16.1.3.</span> </figcaption></figure><figure class="figure-like" id="listing-513"><pre class="console"><b># Set our numpy array from the dataframe

X = np.array(kd[[var1, var2]])
kd['class'] = kd['class'].map({'ckd':1, 'notckd':0, 1:1, 0:0})  # Recode the result feature to be numeric; 
# note the second two values in the dictionary are in case we run this block again

y = np.array(kd[['class']]).reshape(-1)  # had to reshape it to be a vector and not a matrix
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">16.1.4.</span> </figcaption></figure><figure class="figure-like" id="listing-514"><pre class="console"><b>def plot_cat_model(clf, X, y):

    n = X.shape[0]
    test = int(0.20*n)            # We will use a test set made up of 20% of the data from our sample
    perm = rn.permutation(n)   
    X = X[perm]
    y = y[perm]
    X_test = X[:test]       # Then create the test
    y_test = y[:test]
    X_train = X[test:]     # and train sets
    y_train = y[test:]
    
    clf.fit(X_train, y_train)
    
    nn = 200 # Mesh size  
    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, m_max]x[y_min, y_max].
    x_min, x_max = X[:, 0].min(), X[:, 0].max()
    y_min, y_max = X[:, 1].min(), X[:, 1].max()
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nn),
        np.linspace(y_min, y_max, nn))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(8, 6))
    cmap_light = pltco.ListedColormap(['#FFAAAA', '#AAFFAA'])
    cmap_bold = pltco.ListedColormap(['#FF0000', '#00FF00'])
    plt.pcolormesh(xx, yy, Z, cmap=cmap_light);

    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold,marker='o')
    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap_bold, marker='+');
    
    print('Training Error: {}'.format(clf.score(X_train, y_train)))
    print('Testing Error: {}'.format(clf.score(X_test, y_test)))
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">16.1.5.</span> </figcaption></figure><figure class="figure-like" id="listing-515"><pre class="console"><b>from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">16.1.6.</span> </figcaption></figure><figure class="figure-like" id="listing-516"><pre class="console"><b># We define the models that we will combine into an ensemble

clf1 = LogisticRegression(solver='lbfgs')
clf2 = LinearDiscriminantAnalysis()
clf3 = QuadraticDiscriminantAnalysis()
clf4 = SVC(C=1.0, gamma='auto', kernel='linear')
clf5 = SVC(C=1.0, gamma='auto', kernel='rbf')
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">16.1.7.</span> </figcaption></figure><p id="p-903">The ensemble model is declared and the component models are listed. There are three important parameters, in addition to the parameters in the component models (that in this example I set above).</p>
<ul id="p-904" class="disc">
<li id="li-185"><p id="p-905"><em class="emphasis">voting</em> is either 'hard' or 'soft'. Hard voting means that the individual models make their predictions and then the majority selects the classification (this performs best if you have an odd number of models in a two class problem). Soft voting means that the maximum predicted probability on each class determines the value.</p></li>
<li id="li-186"><p id="p-906"><em class="emphasis">weights</em> give different weights to each of the component models. For example we might find the model peforms best when the least flexible model is given a greater proportion of the votes.</p></li>
</ul>
<p id="p-907">Note that the method is parallizable, like the <em class="emphasis">Random Forest</em> method, i.e. each model can be computed indepdently. The parameter <em class="emphasis">n_jobs</em> gives the number of processors that should be dedicated to the computation and setting it to -1 means to use all available processors.</p>
<figure class="figure-like" id="listing-517"><pre class="console"><b>vclf = VotingClassifier(estimators = [ ('lr', clf1), ('lda', clf2), ('qda', clf3), ('lsvc', clf4), ('rsvc', clf5)], 
                       voting='hard', 
                       n_jobs = -1)
plot_cat_model(vclf, X, y)
plt.savefig('16.1.png')
</b>Training Error: 0.9433962264150944
Testing Error: 0.9622641509433962
</pre>
<div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="Images/16.1.png" style="width: 100%; height: auto;" alt=""></div>
<figcaption><span class="type">Listing</span> <span class="codenumber">16.1.8.</span> </figcaption></figure></section><section class="subsection" id="subsection-80"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">16.1.1</span> <span class="title">Baseball Player Data</span>
</h3>
<figure class="figure-like" id="listing-518"><pre class="console"><b># Read in the major league players data set

major = pa.read_csv('Data Sets/Major League Height-Weight.csv')   # creates a pandas Dataframe from a .csv file
major = major.rename({'Height(inches)':'Height', 'Weight(pounds)':'Weight'}, axis=1)  
major = major.drop(640, axis=0) 

# Each row of a data frame is a data point - in this case a player.
# Each column is a feature about the data point (its coordinates if you will)

major.head()   # displays the first few rows
</b>              Name Team       Position  Height  Weight    Age
0    Adam_Donachie  BAL        Catcher      74   180.0  22.99
1        Paul_Bako  BAL        Catcher      74   215.0  34.69
2  Ramon_Hernandez  BAL        Catcher      72   210.0  30.78
3     Kevin_Millar  BAL  First_Baseman      72   210.0  35.43
4      Chris_Gomez  BAL  First_Baseman      73   188.0  35.71
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">16.1.9.</span> </figcaption></figure><figure class="figure-like" id="listing-519"><pre class="console"><b># We are going to make a dictionary of key:value pairs to recode the Position feature to 
# 'Pitcher' and 'Not Pitcher'

pos_dict = {x:1 for x in list(set(major.Position)) if 'Pitcher' in x}
pos_dict2 = {x:0 for x in list(set(major.Position)) if not 'Pitcher' in x}

# Using dictionary comprehensions

pos_dict.update(pos_dict2) # merge the two dictionaries

major_2 = major.copy()   # pandas.dataframe is a mutable object so we use the .copy() command 
                         # otherwise changes to major_2 will change major
    
major_2.Position = major_2.Position.map(pos_dict)  # use the dictionary we made to recode the values with .map()

major_2
</b>                 Name Team  Position  Height  Weight    Age
0       Adam_Donachie  BAL         0      74   180.0  22.99
1           Paul_Bako  BAL         0      74   215.0  34.69
2     Ramon_Hernandez  BAL         0      72   210.0  30.78
3        Kevin_Millar  BAL         0      72   210.0  35.43
4         Chris_Gomez  BAL         0      73   188.0  35.71
...               ...  ...       ...     ...     ...    ...
1029    Brad_Thompson  STL         1      73   190.0  25.08
1030    Tyler_Johnson  STL         1      74   180.0  25.73
1031   Chris_Narveson  STL         1      75   205.0  25.19
1032    Randy_Keisler  STL         1      75   190.0  31.01
1033      Josh_Kinney  STL         1      73   195.0  27.92

[1033 rows x 6 columns]
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">16.1.10.</span> </figcaption></figure><figure class="figure-like" id="listing-520"><pre class="console"><b>X = np.array(major_2[['Height', 'Weight']])
y = np.array(major_2['Position'])
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">16.1.11.</span> </figcaption></figure><figure class="figure-like" id="listing-521"><pre class="console"><b># We define the models that we will combine into an ensemble

clf1 = LogisticRegression(solver='lbfgs')
clf2 = LinearDiscriminantAnalysis()
clf3 = QuadraticDiscriminantAnalysis()
clf4 = SVC(C=1.0, gamma='auto', kernel='linear')
clf5 = SVC(C=1.0, gamma='auto', kernel='rbf')
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">16.1.12.</span> </figcaption></figure><figure class="figure-like" id="listing-522"><pre class="console"><b>vclf = VotingClassifier(estimators = [ ('lr', clf1), ('lda', clf2), ('qda', clf3), ('lsvc', clf4), ('rsvc', clf5)], 
                       voting='hard', 
                       weights = [1, 1, 1.51, 1, 1.5], 
                       n_jobs = -1)
plot_cat_model(vclf, X, y)
plt.savefig('16.2.png')
</b>Training Error: 0.6614268440145102
Testing Error: 0.7038834951456311
</pre>
<div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="Images/16.2.png" style="width: 100%; height: auto;" alt=""></div>
<figcaption><span class="type">Listing</span> <span class="codenumber">16.1.13.</span> </figcaption></figure><p id="p-908">One item to do here is to use the <em class="emphasis">weights</em> as a tuning parameter and determin the best choice for the classification.</p>
<p id="p-909">Note that another possibility is to use a <em class="alert">large</em> collection of models like a set of SVC models with small changes to the parameter. Or collection of different K-Nearest Neighbors with different values of k. Particularly because the algorithm is paralleizable, depending on the hardware you are using, there may be only a small performance penalty for using a large number of models in your ensemble.</p></section></section></div></main>
</div>
<div class="login-link"><span id="loginlogout" class="login">login</span></div>
<script src="https://pretextbook.org/js/0.12/login.js"></script>
</body>
</html>
