<!DOCTYPE html>
<html lang="en-US">
<!--**************************************-->
<!--*    Generated from PreTeXt source   *-->
<!--*    on 2019-09-18T15:31:46-06:00    *-->
<!--*                                    *-->
<!--*      https://pretextbook.org       *-->
<!--*                                    *-->
<!--**************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body>
<article class="paragraphs"><h5 class="heading"><span class="title">Principal Component Analysis.</span></h5>
<p>Scikitlearn contains a variation on this method called Principal Component Analysis that does the computation here, but also normalizes the results in a way that is better suited for data analysis and varies the routines and approximations to run as effeciently as possible on the data you use. It also has some attached functions that can be used to illustrate how well the method has worked.</p>
<figure class="figure-like"><pre class="console"><b>from sklearn.decomposition import PCA
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">10.2.13.</span> </figcaption></figure><figure class="figure-like"><pre class="console"><b>pca = PCA(n_components=2) # Use PCA to identify the two most important components

# component here means a linear combination of the features

Xpca = pca.fit_transform(X);
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">10.2.14.</span> </figcaption></figure><figure class="figure-like"><pre class="console"><b># We can check how the predictors do at classifying flower types by graphing individual pairs for example Sepal_Length and Sepal_Width

plt.plot(Xpca[y==0, 0], Xpca[y==0, 1], 'r.')
plt.plot(Xpca[y==1, 0], Xpca[y==1, 1], 'g.')
plt.plot(Xpca[y==2, 0], Xpca[y==2, 1], 'b.');
</b></pre>
<div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="Images/10.3.png" style="width: 100%; height: auto;" alt=""></div>
<figcaption><span class="type">Listing</span> <span class="codenumber">10.2.15.</span> </figcaption></figure><figure class="figure-like"><pre class="console"><b># The explained variance quantifies the propotion of the variation in the values of X that have been explained by each component

print(pca.explained_variance_ratio_)
</b>[0.92461621 0.05301557]
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">10.2.16.</span> </figcaption></figure><p>Note that PCA is our first example of an unsupervised learning method (it works independently of the result values and rather tries to organize the predictors).</p></article><span class="incontext"><a href="section-45.html#paragraphs-18">in-context</a></span>
</body>
</html>
