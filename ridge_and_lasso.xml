<chapter xml:id='ridge_and_lasso'>
 <title> Ridge and Lasso Regression
</title>

<introduction>
<p>Today we are going to explore two more Linear Regression methods. Next class we will explore additional classification methods: <em>Linear Discriminant Analysis</em>, <em>Support Vector Machines</em></p>

<p>It is worth pausing though and pointing out that we will not have time in this class to go over every method. It is worth reading through the documentation on the scikitlearn help page to see the complete lists of methods for which we have algorithms. The documentation includes a discussion of the mathematical underpinnings of the methods as well as some advice on what problems or types of issues they are meant to work best on.</p>

<p><url href="https://scikit-learn.org/stable/"> scikitlearn documentation </url> </p>

<p>To review: </p>

<p><alert>Advantages of Linear Regression</alert>: </p>
<p><ul>
<li><p>Fast model to train, </p>
</li>
<li><p>founded on mathematical principles from basic courses (<em>Linear Algebra and Calculus</em>), </p>
</li>
<li><p>easy model to interpret (the coeffecients tell us the precise effect of the features)</p>
</li>
<li><p>Rigid model (less chance of overfitting)</p>
</li>
<li><p>Adding additional features improves the <m>R^2</m> for the training data</p>
</li>
</ul></p>

<p><alert>Disadvantages of Linear Regression</alert>:</p>
<p><ul>
<li><p>Assumes linear relationships (can be mitigated in some cases by transforming features to new features)</p>
</li>
<li><p>Requires lots of features if they are categorical</p>
</li>
<li><p>Rigid model (will not respond to nonlinearity)</p>
</li>
<li><p>Adding additional features may not improve the <m>R^2</m> for the testing data</p>
</li>
</ul></p>

<p><alert>A new disadvantage</alert>:</p>
<p><ul>
<li><p>Interpretation becomes hard if the number of features used is very large. </p>
</li>
<li><p>Adding additional features makes the model found more complicated</p>
</li>
</ul></p>

<p>Meaning the algorithm finds the best linear model for the result from the predictors which will mean that it has a tendency to use all of the features and assign a non-zero coefficient to them. This is problematic because:</p>

<p><ol>
<li><p> It makes interpretation difficult because the model developed incorporates every feature;</p>
</li>
<li><p> It also makes the model dependent on less important features and might therefore introduce overfitting depending on the training data.</p>
</li>
</ol></p>

<p>Overall the principal is that given two linear models for our data with comparable errors on the testing data, we would prefer the one that has more coefficients that are zero or near zero. There is nothing in our Linear Regression method that encourages coefficients to not get too large.</p>

<p>We will develop two new models below, based on the principle of linear regression, but with an adjustment that will result in encouraging them to find models with small or zero coefficients. We will see that both models outperform <em>Linear Regression</em> at least in the test case and provided we choose the parameters carefully.</p>

<p>And that last sentence is the real description of what we are doing, we are introducing a parameter to the model that will give us some tuning or adjustment we can make to encourage the model to perform better, similar to the <m>k</m> in <m>k</m>-<em>Nearest Neighbors</em>. Note that <em>Linear Regression</em> had no such thing.</p>
</introduction>

<section>
	<title>House Pricing Data: Linear Regression </title>

<introduction>
<p>The difference between <em>Linear Regression</em> and <em>Ridge or LASSO Regression</em> becomes apparent when the number of features is large. Hence we will work with the House Pricing Data we have seen before, and we need to do some preliminary work to convert a number of categorical features into numerical ones. I stopped before I was completely done but you can get the idea.</p>
<listing>
<console>
<input>
import pandas as pa
import numpy as np
import matplotlib.pyplot as plt
</input>
</console>
</listing>
<listing>
<console>
<input>
# Consider the following dataset about homes that sold in a city in Iowa

hd = pa.read_csv('Data Sets/house-prices/train.csv')

hd.head()
</input>
<output>
   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \
0   1          60       RL         65.0     8450   Pave   NaN      Reg   
1   2          20       RL         80.0     9600   Pave   NaN      Reg   
2   3          60       RL         68.0    11250   Pave   NaN      IR1   
3   4          70       RL         60.0     9550   Pave   NaN      IR1   
4   5          60       RL         84.0    14260   Pave   NaN      IR1   

  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \
0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   
1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   
2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   
3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   
4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   

  YrSold  SaleType  SaleCondition  SalePrice  
0   2008        WD         Normal     208500  
1   2007        WD         Normal     181500  
2   2008        WD         Normal     223500  
3   2006        WD        Abnorml     140000  
4   2008        WD         Normal     250000  

[5 rows x 81 columns]
</output>
</console>
</listing>
<listing>
<console>
<input>
def onehot(df, feature):
    '''A function to do one-hot-encoding of a feature from a dataframe. df = dataframe'''

    v = list(set(df[feature])) # Make an iterable of the unique values for the feature
    
    for c in df.index: # cycle through the samples
        t = df.loc[c, feature]
        
        for test in v:
            if pa.isna(test):  # nan values are sort of a problem and have to be handled separately
                if pa.isna(t):
                    df.loc[c, '{}_nan'.format(feature)] = 1
                else:
                    df.loc[c, '{}_nan'.format(feature)] = 0
            else:
                if t == test:
                    df.loc[c, '{}_{}'.format(feature, test)] = 1  # Makes a new feature with name feature_value
                                                              # and codes it as a 1 if that was the value
                else:
                    df.loc[c, '{}_{}'.format(feature, test)] = 0  # and 0 otherwise
            
    return df.drop(feature, axis=1) # returns a dataframe with the encoded feature removed
</input>
</console>
</listing>
<listing>
<console>
<input>
# We can recode it using a dictionary and .map()
Street_dict = {'Grvl':0, 'Pave':1, 0:0, 1:1}  
# Note we include the trivial coding of the new values as otherwise if we run this twice
# it produces NaN values for the Street feature.
hd.Street = hd.Street.map(Street_dict)
set(hd.Street)
</input>
<output>
{0, 1}
</output>
</console>
</listing>
<listing>
<console>
<input>
hd2 = onehot(hd, 'Alley')
hd3 = onehot(onehot(onehot(hd2, 'ExterQual'), 'LotShape'), 'LandContour')
hd3.head()
</input>
<output>
   Id  MSSubClass MSZoning  LotFrontage  LotArea  Street Utilities LotConfig  \
0   1          60       RL         65.0     8450       1    AllPub    Inside   
1   2          20       RL         80.0     9600       1    AllPub       FR2   
2   3          60       RL         68.0    11250       1    AllPub    Inside   
3   4          70       RL         60.0     9550       1    AllPub    Corner   
4   5          60       RL         84.0    14260       1    AllPub       FR2   

  LandSlope Neighborhood  ... ExterQual_TA ExterQual_Fa LotShape_IR3  \
0       Gtl      CollgCr  ...          0.0          0.0          0.0   
1       Gtl      Veenker  ...          1.0          0.0          0.0   
2       Gtl      CollgCr  ...          0.0          0.0          0.0   
3       Gtl      Crawfor  ...          1.0          0.0          0.0   
4       Gtl      NoRidge  ...          0.0          0.0          0.0   

  LotShape_IR1  LotShape_Reg  LotShape_IR2  LandContour_Lvl  LandContour_Low  \
0          0.0           1.0           0.0              1.0              0.0   
1          0.0           1.0           0.0              1.0              0.0   
2          1.0           0.0           0.0              1.0              0.0   
3          1.0           0.0           0.0              1.0              0.0   
4          1.0           0.0           0.0              1.0              0.0   

  LandContour_Bnk LandContour_HLS  
0             0.0             0.0  
1             0.0             0.0  
2             0.0             0.0  
3             0.0             0.0  
4             0.0             0.0  

[5 rows x 92 columns]
</output>
</console>
</listing>
<listing>
<console>
<input>
Utilities_dict = {'AllPub':1, 'NoSeWa':0, 1:1, 0:0}
hd3.Utilities = hd3.Utilities.map(Utilities_dict)
hd3.head()
</input>
<output>
   Id  MSSubClass MSZoning  LotFrontage  LotArea  Street  Utilities LotConfig  \
0   1          60       RL         65.0     8450       1          1    Inside   
1   2          20       RL         80.0     9600       1          1       FR2   
2   3          60       RL         68.0    11250       1          1    Inside   
3   4          70       RL         60.0     9550       1          1    Corner   
4   5          60       RL         84.0    14260       1          1       FR2   

  LandSlope Neighborhood  ... ExterQual_TA ExterQual_Fa LotShape_IR3  \
0       Gtl      CollgCr  ...          0.0          0.0          0.0   
1       Gtl      Veenker  ...          1.0          0.0          0.0   
2       Gtl      CollgCr  ...          0.0          0.0          0.0   
3       Gtl      Crawfor  ...          1.0          0.0          0.0   
4       Gtl      NoRidge  ...          0.0          0.0          0.0   

  LotShape_IR1  LotShape_Reg  LotShape_IR2  LandContour_Lvl  LandContour_Low  \
0          0.0           1.0           0.0              1.0              0.0   
1          0.0           1.0           0.0              1.0              0.0   
2          1.0           0.0           0.0              1.0              0.0   
3          1.0           0.0           0.0              1.0              0.0   
4          1.0           0.0           0.0              1.0              0.0   

  LandContour_Bnk LandContour_HLS  
0             0.0             0.0  
1             0.0             0.0  
2             0.0             0.0  
3             0.0             0.0  
4             0.0             0.0  

[5 rows x 92 columns]
</output>
</console>
</listing>
<listing>
<console>
<input>
hd4 = onehot(onehot(onehot(onehot(hd3, 'LotConfig'), 'LandSlope'), 'Neighborhood'), 'MSZoning')
hd4.head()
</input>
<output>
   Id  MSSubClass  LotFrontage  LotArea  Street  Utilities Condition1  \
0   1          60         65.0     8450       1          1       Norm   
1   2          20         80.0     9600       1          1      Feedr   
2   3          60         68.0    11250       1          1       Norm   
3   4          70         60.0     9550       1          1       Norm   
4   5          60         84.0    14260       1          1       Norm   

  Condition2 BldgType HouseStyle  ...  Neighborhood_Mitchel  \
0       Norm     1Fam     2Story  ...                   0.0   
1       Norm     1Fam     1Story  ...                   0.0   
2       Norm     1Fam     2Story  ...                   0.0   
3       Norm     1Fam     2Story  ...                   0.0   
4       Norm     1Fam     2Story  ...                   0.0   

   Neighborhood_StoneBr  Neighborhood_SawyerW  Neighborhood_MeadowV  \
0                   0.0                   0.0                   0.0   
1                   0.0                   0.0                   0.0   
2                   0.0                   0.0                   0.0   
3                   0.0                   0.0                   0.0   
4                   0.0                   0.0                   0.0   

  Neighborhood_CollgCr MSZoning_C (all) MSZoning_RH MSZoning_RL MSZoning_FV  \
0                  1.0              0.0         0.0         1.0         0.0   
1                  0.0              0.0         0.0         1.0         0.0   
2                  1.0              0.0         0.0         1.0         0.0   
3                  0.0              0.0         0.0         1.0         0.0   
4                  0.0              0.0         0.0         1.0         0.0   

   MSZoning_RM  
0          0.0  
1          0.0  
2          0.0  
3          0.0  
4          0.0  

[5 rows x 126 columns]
</output>
</console>
</listing>
<listing>
<console>
<input>
def condition(df, feature1, feature2):
    
    #This is onehot encoding but for features that represent the same things 
    # - i.e. where a sample could have two values identified.
    
    v = set(list(set(df[feature1])) + list(set(df[feature2])))  # Build a list of the possible outputs
    
    for c in df.index: # cycle through the samples
        t1 = df.loc[c, feature1]
        t2 = df.loc[c, feature2]
        
        for test in v:
            if (t1==test) or (t2==test):
                df.loc[c, '{}_{}'.format(feature1, test)] = 1  # Makes a new feature with name feature_value
                                                            # and codes it as a 1 if that was the value
            else:
                df.loc[c, '{}_{}'.format(feature1, test)] = 0  # and 0 otherwise
            
    return df.drop([feature1, feature2], axis=1) # returns a dataframe with the encoded feature removed
</input>
</console>
</listing>
<listing>
<console>
<input>
hd5 = condition(hd4, 'Condition1', 'Condition2')
hd5.head()
</input>
<output>
   Id  MSSubClass  LotFrontage  LotArea  Street  Utilities BldgType  \
0   1          60         65.0     8450       1          1     1Fam   
1   2          20         80.0     9600       1          1     1Fam   
2   3          60         68.0    11250       1          1     1Fam   
3   4          70         60.0     9550       1          1     1Fam   
4   5          60         84.0    14260       1          1     1Fam   

  HouseStyle  OverallQual  OverallCond  ...  MSZoning_RM  Condition1_RRNe  \
0     2Story            7            5  ...          0.0              0.0   
1     1Story            6            8  ...          0.0              0.0   
2     2Story            7            5  ...          0.0              0.0   
3     2Story            7            5  ...          0.0              0.0   
4     2Story            8            5  ...          0.0              0.0   

  Condition1_RRAe Condition1_PosN Condition1_Feedr Condition1_Artery  \
0             0.0             0.0              0.0               0.0   
1             0.0             0.0              1.0               0.0   
2             0.0             0.0              0.0               0.0   
3             0.0             0.0              0.0               0.0   
4             0.0             0.0              0.0               0.0   

  Condition1_PosA  Condition1_RRAn Condition1_RRNn Condition1_Norm  
0             0.0              0.0             0.0             1.0  
1             0.0              0.0             0.0             1.0  
2             0.0              0.0             0.0             1.0  
3             0.0              0.0             0.0             1.0  
4             0.0              0.0             0.0             1.0  

[5 rows x 133 columns]
</output>
</console>
</listing>
<listing>
<console>
<input>
hd6 = onehot(onehot(onehot(onehot(hd5, 'BldgType'), 'HouseStyle'), 'RoofStyle'), 'RoofMatl')
hd6.iloc[:5, 9:]
</input>
<output>
   YearRemodAdd Exterior1st Exterior2nd MasVnrType  MasVnrArea ExterCond  \
0          2003     VinylSd     VinylSd    BrkFace       196.0        TA   
1          1976     MetalSd     MetalSd       None         0.0        TA   
2          2002     VinylSd     VinylSd    BrkFace       162.0        TA   
3          1970     Wd Sdng     Wd Shng       None         0.0        TA   
4          2000     VinylSd     VinylSd    BrkFace       350.0        TA   

  Foundation BsmtQual BsmtCond BsmtExposure  ... RoofStyle_Shed  \
0      PConc       Gd       TA           No  ...            0.0   
1     CBlock       Gd       TA           Gd  ...            0.0   
2      PConc       Gd       TA           Mn  ...            0.0   
3     BrkTil       TA       Gd           No  ...            0.0   
4      PConc       Gd       TA           Av  ...            0.0   

   RoofStyle_Mansard RoofMatl_CompShg  RoofMatl_Membran  RoofMatl_Roll  \
0                0.0              1.0               0.0            0.0   
1                0.0              1.0               0.0            0.0   
2                0.0              1.0               0.0            0.0   
3                0.0              1.0               0.0            0.0   
4                0.0              1.0               0.0            0.0   

   RoofMatl_ClyTile RoofMatl_Tar&amp;Grv RoofMatl_Metal RoofMatl_WdShake  \
0               0.0              0.0            0.0              0.0   
1               0.0              0.0            0.0              0.0   
2               0.0              0.0            0.0              0.0   
3               0.0              0.0            0.0              0.0   
4               0.0              0.0            0.0              0.0   

  RoofMatl_WdShngl  
0              0.0  
1              0.0  
2              0.0  
3              0.0  
4              0.0  

[5 rows x 147 columns]
</output>
</console>
</listing>
<listing>
<console>
<input>
# Probably enough, lets make a list of all of the features that are not 'object'
# (i.e. that are numerical)

keep = hd6.columns[hd6.dtypes!='object'][1:]
keep
</input>
<output>
Index(['MSSubClass', 'LotFrontage', 'LotArea', 'Street', 'Utilities',
       'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea',
       ...
       'RoofStyle_Shed', 'RoofStyle_Mansard', 'RoofMatl_CompShg',
       'RoofMatl_Membran', 'RoofMatl_Roll', 'RoofMatl_ClyTile',
       'RoofMatl_Tar&amp;Grv', 'RoofMatl_Metal', 'RoofMatl_WdShake',
       'RoofMatl_WdShngl'],
      dtype='object', length=128)
</output>
</console>
</listing>
<p>So we have 128 features. That is a lot, and should be enough to illustrate the difference between <em>Linear Regression</em> and the new methods.</p>
<listing>
<console>
<input>
# Check for NaN

for v in keep:
    if sum(np.isnan(hd6[v]))!=0:
        print(v)
</input>
<output>
LotFrontage
MasVnrArea
GarageYrBlt

</output>
</console>
</listing>
<listing>
<console>
<input>
# MasVnrArea and GarageYrBlt that are NaN probably mean 0 (i.e. no Garage and no Masonry)

for k in hd6.index:
    if np.isnan(hd6.loc[k, 'MasVnrArea']):
        hd6.loc[k, 'MasVnrArea']=0
    if np.isnan(hd6.loc[k, 'GarageYrBlt']):
        hd6.loc[k, 'GarageYrBlt']=0

</input>
</console>
</listing>
</introduction>

<subsection>
 <title> Linear Regression</title>
<listing>
<console>
<input>
keep = list(hd6.columns[hd6.dtypes!='object'][1:])
keep.remove('LotFrontage')   # There are some NaNs in LotFrontage. We could try to fill them in with a regression.
                             # Or we might check with a content expert and see if they should be 0
hd7 = hd6.loc[:, keep]
hd7.head()
</input>
<output>
   MSSubClass  LotArea  Street  Utilities  OverallQual  OverallCond  \
0          60     8450       1          1            7            5   
1          20     9600       1          1            6            8   
2          60    11250       1          1            7            5   
3          70     9550       1          1            7            5   
4          60    14260       1          1            8            5   

   YearBuilt  YearRemodAdd  MasVnrArea  BsmtFinSF1  ...  RoofStyle_Shed  \
0       2003          2003       196.0         706  ...             0.0   
1       1976          1976         0.0         978  ...             0.0   
2       2001          2002       162.0         486  ...             0.0   
3       1915          1970         0.0         216  ...             0.0   
4       2000          2000       350.0         655  ...             0.0   

   RoofStyle_Mansard  RoofMatl_CompShg  RoofMatl_Membran  RoofMatl_Roll  \
0                0.0               1.0               0.0            0.0   
1                0.0               1.0               0.0            0.0   
2                0.0               1.0               0.0            0.0   
3                0.0               1.0               0.0            0.0   
4                0.0               1.0               0.0            0.0   

   RoofMatl_ClyTile  RoofMatl_Tar&amp;Grv  RoofMatl_Metal  RoofMatl_WdShake  \
0               0.0               0.0             0.0               0.0   
1               0.0               0.0             0.0               0.0   
2               0.0               0.0             0.0               0.0   
3               0.0               0.0             0.0               0.0   
4               0.0               0.0             0.0               0.0   

   RoofMatl_WdShngl  
0               0.0  
1               0.0  
2               0.0  
3               0.0  
4               0.0  

[5 rows x 127 columns]
</output>
</console>
</listing>
<p>Okay so there is our data frame. We start by seeing how <em>Linear Regression</em> does.</p>
<listing>
<console>
<input>
# Convert them to Numpy Arrays X for predictors and y for result

keep.remove('SalePrice')
X = np.array(hd7.loc[:, keep])
y = np.array(hd7.loc[:, 'SalePrice'])
</input>
</console>
</listing>
<listing>
<console>
<input>
import numpy.random as rn
from sklearn.linear_model import LinearRegression
</input>
</console>
</listing>
<listing>
<console>
<input>
# We shuffle the data using a random permutation

n = X.shape[0]
test = int(0.20*n)            # We will use a test set made up of 20% of the data from our sample
perm = rn.permutation(n)   
X = X[perm]
y = y[perm]
X_test = X[:test]       # Then create the test
y_test = y[:test]
X_train = X[test:]     # and train sets
y_train = y[test:]
</input>
</console>
</listing>
<listing>
<console>
<input>
reg = LinearRegression().fit(X_train, y_train)
print('Training R2: {}'.format(reg.score(X_train, y_train)))
print('Testing R2: {}'.format(reg.score(X_test, y_test)))
</input>
<output>
Training R2: 0.9054345997225262
Testing R2: 0.827383244663398

</output>
</console>
</listing>
<p>Reasonably good on the training data, not so good on the testing data (again we really should do some cross validation or bootstrapping to fully explore it).  However, lets check a few things to see how the model is making its prediction, i.e. what our interpretation is.</p>
<listing>
<console>
<input>
plt.plot(reg.coef_);
</input>
</console>
<image source='Images/12.1.png'/>
</listing>
<p>One feature stands out as particularly important.</p>
<listing>
<console>
<input>
# numpy.argmin() returns the index of the minimum value
keep[reg.coef_.argmin()] 
# It's position in keep is then the name of that feature
</input>
<output>
'RoofMatl_ClyTile'
</output>
</console>
</listing>
<listing>
<console>
<input>
keep[reg.coef_.argmax()]
</input>
<output>
'RoofMatl_WdShngl'
</output>
</console>
</listing>
<p><em>side remark</em>: I would not have predicted that Wood Shingle was the most important positive feature.</p>

<p>We also can check the size of the smallest absolute coefficient and the largest absolute coefficient:</p>
<listing>
<console>
<input>
min(abs(reg.coef_)), max(abs(reg.coef_))
</input>
<output>
(0.0, 529309.0018963504)
</output>
</console>
</listing>
<p>Some coefficients were zero, we can ask for how many they were actually zero:</p>
<listing>
<console>
<input>
sum( abs(reg.coef_)==0.0 )
</input>
<output>
2
</output>
</console>
</listing>
<p>Not many. And how many were small, say within 1 of zero (which compared to the largest coefficient is small):</p>
<listing>
<console>
<input>
sum( abs(reg.coef_)&lt;1)
</input>
<output>
6
</output>
</console>
</listing>
</subsection>

</section>
<section>
 <title> Ridge Regression
</title>

<p>Recall that <em>Linear Regression</em> seeks to minimize the sum of square residuals between the data <m>(x_i, y_i)</m> and the linear prediction <m>\hat{y}_i = m_0 x_{i0} + m_1 x_{i1} + \dots m_{p-1} x_{i, p-1} + b</m>:</p>

<p><me> E(m, b) = \sum_{i=0}^{n-1} (y_i - \hat{y}_i)^2</me></p>

<p>From this formulation, there is no penalty for the algorithm choosing coefficients <m>m_j</m> that are big.</p>
<p>What we would like to do is introduce a penalty to <m>E(m, b)</m> for choosing coefficients <m>m_j</m> that are too big. Ridge Regression using the simple idea of introduce a penalty that is a sum of the squares of the <m>m_j</m>:</p>

<p><me> E_{\alpha}(m, b) = \sum_{i=0}^{n-1} (y_i - \hat{y}_i)^2 + \alpha \sum_{j=0}^{p-1} m_j^2</me></p>

<p>First note that <m>\alpha > 0</m> is a parameter that needs to be choosen. <alert>Question</alert>: How will we choose it? </p>

<p>Then the effect of the <m>\alpha</m> term is to increase <m>E</m> if the choices for <m>m_j</m> are large, i.e. it is a penalty for a Linear Model that uses larger coefficients. <m>\alpha</m> effectively chooses how strong that penalty is.</p>

<p><em>Note</em>: We choose the term <m>m_j^2</m> so that it is differentiable with respect <m>m_j</m> and thus the problem is still ammenable to our Calculus 1 approach to linear regression and we can find an exact model to a given training data.</p>
<listing>
<console>
<input>
from sklearn.linear_model import Ridge
</input>
</console>
</listing>
<listing>
<console>
<input>
ridge_reg = Ridge(alpha=1).fit(X_train, y_train)
print('Training R2: {}'.format(ridge_reg.score(X_train, y_train)))
print('Testing R2: {}'.format(ridge_reg.score(X_test, y_test)))
</input>
<output>
Training R2: 0.8945252671230364
Testing R2: 0.8478669227705219

</output>
</console>
</listing>
<p>Reasonably good performance on the training data, somewhat less good on the testing data. The model does outperform <em>Linear Regression</em>. If you are following along with this discussion in Jupyter, you should try different values of <m>\alpha</m> to see how the model responds. In production what we would really want to set up is a <em>Cross Validation</em> or <em>Bootstrap Procedure</em> to compare different choices of <m>\alpha</m> across a range of testing and training sets.</p>
<listing>
<console>
<input>
ridge_reg = Ridge(alpha=30).fit(X_train, y_train)
print('Training R2: {}'.format(ridge_reg.score(X_train, y_train)))
print('Testing R2: {}'.format(ridge_reg.score(X_test, y_test)))
</input>
<output>
Training R2: 0.8592062290180964
Testing R2: 0.8539740519116447

</output>
</console>
</listing>
<p>Check the size of the coefficients.</p>
<listing>
<console>
<input>
plt.plot(ridge_reg.coef_);
</input>
</console>
<image source='Images/12.2.png'/>
</listing>
<listing>
<console>
<input>
# numpy.argmin() returns the index of the maximum value
keep[ridge_reg.coef_.argmax()] 
# It's position in keep is then the name of that feature
</input>
<output>
'Neighborhood_NridgHt'
</output>
</console>
</listing>
<p>Interestingly this model (if you have choosen a large <m>\alpha</m>) gives a different feature as the most important.</p>

<p>The largest absolute coefficient from the Ridge model is much smaller than that from the Linear Regression model.</p>
<listing>
<console>
<input>
min(abs(ridge_reg.coef_)), max(abs(ridge_reg.coef_))
</input>
<output>
(0.0, 24738.435068752682)
</output>
</console>
</listing>
<listing>
<console>
<input>
sum(reg.coef_**2) - sum(ridge_reg.coef_**2)
</input>
<output>
356349170311.3535
</output>
</console>
</listing>
<p>However not many coefficients are exactly 0 (again the result will vary as your testing and training sets change). And indeed about the same number of coefficients are small for this model.</p>
<listing>
<console>
<input>
sum(abs(ridge_reg.coef_)==0.0)
</input>
<output>
3
</output>
</console>
</listing>
<listing>
<console>
<input>
sum(abs(ridge_reg.coef_)&lt;1)
</input>
<output>
6
</output>
</console>
</listing>

</section>
<section>
 <title> Lasso Regression
</title>

<p>So <em>Ridge Regression</em> did some of what we wanted. It produced a Linear Model that had overall smaller coefficients, and in the process that model while it did worse on the training data (which it had to, remember that <em>Linear Regression</em> chooses the model that will do the best out of any linear function at predicting the training data) it performed better on the testing data (though keeping in mind we really should test that statement more carefully). The model was also tuneable, we could choose an <m>\alpha</m> that produced a blance between training and testing <m>R^2</m> values (though again we are skipping the bootstrapping step).</p>

<p>However it still (in most cases I've run) produced a model with quite a few coefficients that were non-zero. We would prefer models where as many coefficients are zero while still performing well because we expect it will produce a model that is easier to interpret.</p>

<p>The idea with <em>Lasso Regression</em> is to introduce a new objective function that will include a different penalty that responds faster to small values of the coefficients, thus encouraging them to go to zero. </p>
<p>We will choose a model that is not differentiable in the <m>m_j</m> coefficients, and thus it will need to use a different solution method than our Calculus 1 approach to <em>Linear and Ridge Regression</em>. Define the new objective function:</p>

<p><me> E_{\alpha}(m, b) = \frac{1}{2n} \sum_{i=0}^{n-1} (y_i - \hat{y}_i)^2 + \alpha \sum_{j=0}^{p-1} | m_j |</me></p>

<p>Note that because we are dividing the first term by the number of samples this also puts more pressure on the penalty term. This objective function cannot be minimized by solving it exactly using Calculus 1 techniques. Instead what the algorithm does is search for a solution by choosing values of <m>m_j</m> and <m>b</m> that improve on the current value, a form of gradient descent. This is therefore our first example of a model that does not give an exact result from a given training set.</p>
<listing>
<console>
<input>
from sklearn.linear_model import Lasso
</input>
</console>
</listing>
<listing>
<console>
<input>
lasso_reg = Lasso(alpha=5).fit(X_train, y_train)
print('Training R2: {}'.format(lasso_reg.score(X_train, y_train)))
print('Testing R2: {}'.format(lasso_reg.score(X_test, y_test)))
</input>
<output>
Training R2: 0.9053703923972203
Testing R2: 0.8429099334384119

</output>
</console>
</listing>
<p>Because the algorithm is using an approximation method to arrive at the model, if your <m>\alpha</m> is too small you will get a warning that the solution did not converge in the maximum number of iterations. You should either increase <m>\alpha</m> until you do not see this error, or increase the maximum number of iterations by adding that parameter to the call of the model.</p>

<p>Note the model performed well on the training data, but underperformed <em>Ridge Regression</em> on the testing data (for this choice of <m>\alpha</m>). However you know what needs to be done, we need to search for a better choice of <m>\alpha</m>. </p>
<listing>
<console>
<input>
lasso_reg = Lasso(alpha=325).fit(X_train, y_train)
print('Training R2: {}'.format(lasso_reg.score(X_train, y_train)))
print('Testing R2: {}'.format(lasso_reg.score(X_test, y_test)))
</input>
<output>
Training R2: 0.8563912506891203
Testing R2: 0.8518055277873428

</output>
</console>
</listing>
<listing>
<console>
<input>
plt.plot(lasso_reg.coef_);
</input>
</console>
<image source='Images/12.3.png'/>
</listing>
<listing>
<console>
<input>
min(abs(lasso_reg.coef_)), max(abs(lasso_reg.coef_))
</input>
<output>
(0.0, 53467.392667787)
</output>
</console>
</listing>
<p>The coefficients are small, and many of them are 0 compared with the <em>Linear and Ridge Regression</em> models. Note that the performance of the model on the test set is comparable with those models. </p>
<listing>
<console>
<input>
sum(abs(lasso_reg.coef_)==0.0)
</input>
<output>
70
</output>
</console>
</listing>
<listing>
<console>
<input>
sum(abs(lasso_reg.coef_)&lt;1)
</input>
<output>
73
</output>
</console>
</listing>



</section>
</chapter>