<chapter xml:id='unsupervised'>
  <title> Unsupervised Learning
</title>

<introduction>
<p>The bulk of this course has been about data problems where we have a set of samples with some predictor features and a result. The goal has been to develop models that will use the values of the features to make a prediction about the result. However in many cases what we have is a set of samples and values for some of their features but with nothing else. This is an example of what we would call an unsupervised learning problem. The goal is to develop algorithms that will help us make sense out of datasets like this. </p>

<p>In the best cases these algorithms will lead to hypothesis from the data about. </p>

<p>We have already seen one example of an unsupervised learning algorithm in the form of a pre-processing algorithm:  <em>Principal Component Analysis</em> looks at a set of features and tries to choose a linear combination of them that does the best job at predicting the variations from sample to sample. If you look back at how we implemented it you will see that it only depended on the predictors and did not use the result values.</p>

<p>Today we will look at a type of unsupervised learning algorithm called <em>Clustering</em> however it is worth noting that there are other approaches to unsupervised learning particularly to developing visualizations for exploring data (see <em>STAT 202</em>) that we will not be discussing in this class. This area of machine learning is definitly one in which research is ongoing and in the next few years there will be many PhD thesis written about unsupervised learning problems.</p>
</introduction>

<section>
 <title> Clustering
</title>

<introduction>
<p>The goal of clustering algorithms is to separate the samples into groups or clusters by the values of their features. A classic use case of this in studying the genomic variety of a species. For example in tracing the process by which an agricultural plant (such as <em>Cannabis Sativa</em> or <em>Humulus Lupulus</em>) has been domesticated one might collect the proportions of various codes included in their DNA for samples of the plants from a wide geographic region and then ask if there are identifiable subgroups within the species.</p>

<p>Another classic use of clustering is by companies in developing targeted marketing. Data is collected from existing customers, machine learning applied to see if indentifiable subgroups of customers can be found and then the motivations behind those types of customers explored to develop marketing strategies for them. Our university is doing this as we speak.</p>

<p>To some extent clustering is still where there is some human needed in deciding if the algorithm returned a useful result or not.</p>
</introduction>

<subsection>
 <title> K-Means
</title>

<p>Our first clustering algorithm is called K-Means. The algorithm will take k, the number of clusters, as a parameter, and then will find the best division of the data into k clusters. </p>

<p>First we need to define the <em>centroid</em> of a subset of our samples to be the coordinates in the feature space given by the means of each of the features over the subset. </p>

<p><alert>The K-Means Algorithm</alert></p>

<p><ol>
<li><p> Starting from a random assignment of each of the samples into on of the k clusters.</p>
</li>
</ol></p>

<p><ol>
<li><p> Proceeding with the following steps until the assignment into Clusters stops changing:</p>
</li>
</ol></p>
<p>    a. Compute the <em>centroid</em> of each cluster form the current assignments.</p>
<p>    b. Reassign the smaples to the clusters by setting it to match the closest centroid.</p>
<p>    </p>
<p>Note right away that because we are using <em>distance</em> in the algorithm the method will produce different results depending on whether we have normalized our data first or not.</p>



<listing>
<console>
<input>
import pandas as pa
import numpy as np
import matplotlib.pyplot as plt
import numpy.random as rnd
import matplotlib.cm as cm
import matplotlib.colors as pltco
</input>
</console>
</listing>
<p>Today we will circle back around to the Colorado Childcare dataset. This dataset contains the geographic location of licensed childcare facilities in Colorado, and we suspect from our earlier analysis that there are clusters within the data. It is a bit problematic and we will see that the K-Means Algorithm misses some of the features.</p>
<listing>
<console>
<input>
df = pa.read_csv('Data Sets/Colorado_Licensed_Child_Care_Facilities_Report.csv')
df.head()
</input>
<output>
   PROVIDER ID                          PROVIDER NAME  \
0           48                          VIKKI MCKEOGH   
1           65                            JACKIE GRAY   
2          100  CHEROKEE TRAIL ELEMENTARY KIDS CENTER   
3          115              EARLY CHILDHOOD EDUCATION   
4          157                     CHERLLYNN SAUNDERS   

                PROVIDER SERVICE TYPE         STREET ADDRESS  \
0  Experienced Family Child Care Home       6635 E Monaco DR   
1              Family Child Care Home          4388 118 Ave.   
2        School-Age Child Care Center  17302 Clarke Farms DR   
3                   Preschool Program         1023 N 31st ST   
4                 Infant/Toddler Home         5989 W Fair DR   

               CITY STATE    ZIP     COUNTY            COMMUNITY  \
0          Brighton    CO  80602      Adams           West Adams   
1          Thornton    CO  80233      Adams           West Adams   
2            Parker    CO  80134    Douglas               Parker   
3  Colorado Springs    CO  80904    El Paso     Colorado Springs   
4         Littleton    CO  80123  Jefferson  Northeast Jefferson   

                                           ECC  ... CCCAP CASE COUNT_D1  \
0  Early Childhood Partnership of Adams County  ...                 NaN   
1  Early Childhood Partnership of Adams County  ...                 3.0   
2       Douglas County Early Childhood Council  ...                 3.0   
3                            Alliance for Kids  ...                 NaN   
4                Triad Early Childhood Council  ...                 NaN   

  CCCAP FA EXP DATE_D1 CCCAP TOTAL AUTH_D1 CCCAP FA STATUS_D1  \
0                  NaN                 NaN                NaN   
1           06/30/2019                 3.0                1.0   
2           06/30/2019                 5.0                1.0   
3                  NaN                 NaN                NaN   
4                  NaN                 NaN                NaN   

  CCCAP AMOUNT PAID_D1  CCCAP FA EXP DATE_D2  CCCAP TOTAL AUTH_D2  \
0                  NaN                   NaN                  NaN   
1               239.20            06/14/2019                  3.0   
2               974.61            05/31/2020                  5.0   
3                  NaN                   NaN                  NaN   
4                  NaN                   NaN                  NaN   

   CCCAP FA STATUS_D2 LICENSE FEE DISCOUNT                  LONG-LAT  
0                 NaN                  0.0  (39.919258, -104.911005)  
1                 1.0                  0.0      (39.9101, -104.9344)  
2                 1.0                  0.0  (39.525183, -104.786646)  
3                 NaN                  0.0   (38.86547, -104.867475)  
4                 NaN                  0.0  (39.604151, -105.063248)  

[5 rows x 27 columns]
</output>
</console>
</listing>
<listing>
<console>
<input>
temp = df.loc[:, ['PROVIDER SERVICE TYPE', 'LONG-LAT'] ]

# pull up the longitude and lattitude coordinates
 
lon = [eval(temp.loc[c, 'LONG-LAT'])[0] for c in temp.index.values]
lat = [eval(temp.loc[c, 'LONG-LAT'])[1] for c in temp.index.values]
    
# Separate the longitude and lattide coordinates
# Some notes on the Python here:  the coordinates have been read in as a string, we can convert them to a 
# tuple with eval and then reference each with a position index.

locate = pa.DataFrame(np.array( [lat, lon]).transpose(), columns = ['Lat', 'Long'])
</input>
</console>
</listing>
<listing>
<console>
<input>
X = np.array(locate)
</input>
</console>
</listing>
<p>As described above, we expect better results after a normalization, be warry though as the types could be integer rather than float. In this case I check that they have been read as floats.</p>
<listing>
<console>
<input>
X.dtype
</input>
<output>
dtype('float64')
</output>
</console>
</listing>
<listing>
<console>
<input>
for l in range(X.shape[1]):
    X[:, l] = ( X[:, l] - X[:, l].min())/(X[:, l].max() - X[:, l].min())
</input>
</console>
</listing>
<listing>
<console>
<input>
plt.plot(X[:, 0], X[:, 1], 'b.');
plt.savefig('17.1.png')
</input>
</console>
<image source='Images/17.1.png'/>
</listing>
<p>We will do the K-Means Clustering by hand here and then introduce the scikit learn package implementing it.</p>

<p>Let's see if we can identify two clusters in this data. We start by randomly assigning each point to one of the two groups.</p>
<listing>
<console>
<input>
y = np.array(rnd.randint(2, size=X.shape[0]))
</input>
</console>
</listing>
<listing>
<console>
<input>
plt.plot(X[y==0, 0], X[y==0, 1], 'b.')
plt.plot(X[y==1, 0], X[y==1, 1], 'r.')
plt.savefig('17.2.png')
</input>
</console>
<image source='Images/17.2.png'/>
</listing>
<p>For each group we compute the centroid by finding the mean of the values along each axis.</p>
<listing>
<console>
<input>
def ret_centroids(X, y):
    
    return np.array([[X[y==0, 0].mean(), X[y==0, 1].mean()], [X[y==1, 0].mean(), X[y==1, 1].mean()]])
</input>
</console>
</listing>
<listing>
<console>
<input>
c = ret_centroids(X, y)
c
</input>
<output>
array([[0.54840172, 0.63470342],
       [0.54462971, 0.63159221]])
</output>
</console>
</listing>
<listing>
<console>
<input>
def plot_cluster(X, y, c):

    plt.figure(figsize=(10, 8))
    plt.plot(X[y==0, 0], X[y==0, 1], 'b.')
    plt.plot(X[y==1, 0], X[y==1, 1], 'r.')
    plt.plot(c[0, 0], c[0, 1], 'b*')
    plt.plot(c[1, 0], c[1, 1], 'r*')
    
    return None
</input>
</console>
</listing>
<listing>
<console>
<input>
plot_cluster(X, y, c)
plt.savefig('17.3.png')
</input>
</console>
<image source='Images/17.3.png'/>
</listing>
<p>They are there (very close to each other).</p>

<p>Now we reassign the values of each point based on the centroid they are closest to.</p>

<listing>
<console>
<input>
def assign_cluster(X, y, c):
    
    for k in range(X.shape[0]):
        d0 = np.sqrt( (X[k, 0] - c[0, 0])**2 + (X[k, 1] - c[0, 1])**2)
        d1 = np.sqrt( (X[k, 0] - c[1, 0])**2 + (X[k, 1] - c[1, 1])**2)
        
        if d0 &lt;= d1:
            y[k] = 0
        else:
            y[k] = 1
            
    return y
</input>
</console>
</listing>
<listing>
<console>
<input>
y = assign_cluster(X, y, c)
plot_cluster(X, y, c)
plt.savefig('17.4.png')
</input>
</console>
<image source='Images/17.4.png'/>
</listing>
<listing>
<console>
<input>
c = ret_centroids(X, y)
plot_cluster(X, y, c)
plt.savefig('17.5.png')
</input>
</console>
<image source='Images/17.5.png'/>
</listing>
<listing>
<console>
<input>
y = assign_cluster(X, y, c)
plot_cluster(X, y, c)
plt.savefig('17.6.png')
</input>
</console>
<image source='Images/17.6.png'/>
</listing>
<listing>
<console>
<input>
c = ret_centroids(X, y)
plot_cluster(X, y, c)
plt.savefig('17.7.png')
</input>
</console>
<image source='Images/17.7.png'/>
</listing>
<listing>
<console>
<input>
y = assign_cluster(X, y, c)
plot_cluster(X, y, c)
plt.savefig('17.8.png')
</input>
</console>
<image source='Images/17.8.png'/>
</listing>
<p>In practice K-means is minimizing the within-cluster sum-of-squares criterion that is the sum of the distances between the samples of a cluster and its centroid. This measurement does have some drawbacks, it will favor clusters that are convex, and it will miss clusters that are elongated or have irregular shapes.</p>

<p>Evaluating the performance of a clustering model is not as straightforward as computing the percentage of correct classifications that we do in supervised learning. In this case we do not know what the true classification is for our sample.</p>

<p>The main parameter in the algorithm is <m>K</m>, the number of clusters. </p>
<listing>
<console>
<input>
def cluster_plot(cluster, X):
       
    n_clusters = cluster.get_params()['n_clusters']
    cmap_bold = pltco.ListedColormap([cm.hot(k/n_clusters) for k in range(n_clusters) ])  
    # We need a color map that expands to match the number of clusters

    plt.scatter(X[:, 0], X[:, 1], c=cluster.fit_predict(X), cmap=cmap_bold,marker='o')
</input>
</console>
</listing>
<listing>
<console>
<input>
from sklearn.cluster import KMeans
</input>
</console>
</listing>
<listing>
<console>
<input>
cluster = KMeans(n_clusters=2)
cluster_plot(cluster, X)
plt.savefig('17.9.png')
</input>
</console>
<image source='Images/17.9.png'/>
</listing>
<listing>
<console>
<input>
cluster = KMeans(n_clusters=3)
cluster_plot(cluster, X)
plt.savefig('17.10.png')
</input>
</console>
<image source='Images/17.10.png'/>
</listing>
<listing>
<console>
<input>
cluster = KMeans(n_clusters=4)
cluster_plot(cluster, X)
plt.savefig('17.11.png')
</input>
</console>
<image source='Images/17.11.png'/>
</listing>
<listing>
<console>
<input>
cluster = KMeans(n_clusters=5)
cluster_plot(cluster, X)
plt.savefig('17.12.png')
</input>
</console>
<image source='Images/17.12.png'/>
</listing>
<listing>
<console>
<input>
cluster = KMeans(n_clusters=6)
cluster_plot(cluster, X)
plt.savefig('17.13.png')
</input>
</console>
<image source='Images/17.13.png'/>
</listing>
<listing>
<console>
<input>
cluster = KMeans(n_clusters=8)
cluster_plot(cluster, X)
plt.savefig('17.14.png')
</input>
</console>
<image source='Images/17.14.png'/>
</listing>
<p>The algorithm has produced a rather unsurprising clustering of the states childcare facilities that more or less corresponds to the major geographic and demographic regions. Note that we have not taken into account any of the other features in the data.</p>

<p>Clustering is a tool for teasing out meaning in the data or identifying hypothesis that deserve exploring.</p>

</subsection>

<subsection>
 <title> Hierarchical Clustering
</title>

<p><em>Hierarchical Clustering</em> are algorithms that decide on clusters among the data by looking at merging or splitting cluster divisions. The are different from the distance based clustering algorithms used above and support some generalizations that are capable of picking out clusters with odd geometric structure.</p>

<p>These clusters are also capable of identifing groupsings that have uneven cluster sizes.</p>

<p>The objective function we will be minimizing is the <em>linkage distance</em> this is one of the parameters for the algorithm that can be varied it can be set to the 'single', 'average', 'complete', or 'Ward'. Furthermore the actual distance function underlying the linkage distance can be varried as well. The linkage distance used will affect the types of clusters that will be identified. See scikit-learn for a complete overview.</p>

<p>The algorithm we will examine today, <em>Agglomerative Clustering</em>, is a bottom up approach. It will start with each data point from the sample in its own cluster and then proceed to merge pairs of clusters that will give a minimal increase in the <em>linkage distance</em>.</p>
<listing>
<console>
<input>
from sklearn.cluster import AgglomerativeClustering
</input>
</console>
</listing>
<listing>
<console>
<input>
cluster = AgglomerativeClustering(n_clusters=2, linkage='ward')
cluster_plot(cluster, X)
plt.savefig('17.15.png')
</input>
</console>
<image source='Images/17.15.png'/>
</listing>
<listing>
<console>
<input>
cluster = AgglomerativeClustering(n_clusters=3, linkage='ward')
cluster_plot(cluster, X)
plt.savefig('17.16.png')
</input>
</console>
<image source='Images/17.16.png'/>
</listing>
<listing>
<console>
<input>
cluster = AgglomerativeClustering(n_clusters=4, linkage='ward')
cluster_plot(cluster, X)
plt.savefig('17.17.png')
</input>
</console>
<image source='Images/17.17.png'/>
</listing>
<listing>
<console>
<input>
cluster = AgglomerativeClustering(n_clusters=5, linkage='ward')
cluster_plot(cluster, X)
plt.savefig('17.18.png')
</input>
</console>
<image source='Images/17.18.png'/>
</listing>
<p>Changing the linkage used adjusts the shapes and types of clusters that are identified.</p>
<listing>
<console>
<input>
cluster = AgglomerativeClustering(n_clusters=5, linkage='complete')
cluster_plot(cluster, X)
plt.savefig('17.19.png')
</input>
</console>
<image source='Images/17.19.png'/>  
</listing>
<listing>
<console>
<input>
cluster = AgglomerativeClustering(n_clusters=5, linkage='average')
cluster_plot(cluster, X)
plt.savefig('17.20.png')
</input>
</console>
<image source='Images/17.20.png'/>
</listing>
<listing>
<console>
<input>
cluster = AgglomerativeClustering(n_clusters=5, linkage='single')
cluster_plot(cluster, X)
plt.savefig('17.21.png')
</input>
</console>
<image source='Images/17.21.png'/>
</listing>
<p>This last example illustrates another use of clustering. It can identify samples that are significantly separated in the feature space from the rest of the data set.</p>
</subsection>

<subsection>
 <title> Birch Algorithm
</title>

<p>The <em>Birch</em> algorithm builds a tree called the Characteristic Feature Tree for the data. The data is compressed into the feature tree and the algorithm stores the necessary information without keeping the entire data set used. Each node of the tree includes information about the number of samples it represents and their relatives distances and location in the feature space. There are two parameters in the algorithm a <em>threshhold</em> and a <em>branching factor</em>. One use of Birch is to reduce the size of the dataset being used as that is its primary approach in identifying samples that are closely related. It can be passed the number of clusters or it can produce as a result the number of clusters.</p>
<listing>
<console>
<input>
from sklearn.cluster import Birch
</input>
</console>
</listing>
<listing>
<console>
<input>
cluster = Birch(threshold = 0.10, n_clusters=5)
cluster_plot(cluster, X)
plt.savefig('17.22.png')
</input>
</console>
<image source='Images/17.22.png'/>
</listing>
<listing>
<console>
<input>
cluster = Birch(threshold = 0.10, n_clusters=None)
cluster.fit(X)
labels = set(cluster.labels_)
labels
</input>
<output>
{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}
</output>
</console>
</listing>
<listing>
<console>
<input>
n_clusters = len(labels)
cmap_bold = pltco.ListedColormap([cm.tab20b(k/n_clusters) for k in range(n_clusters) ])  
# We need a color map that expands to match the number of clusters

plt.scatter(X[:, 0], X[:, 1], c=cluster.labels_, cmap=cmap_bold,marker='o');
plt.savefig('17.23.png')
</input>
</console>
<image source='Images/17.23.png'/>
</listing>

</subsection>

</section>
</chapter>