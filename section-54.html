<!DOCTYPE html>
<!--**************************************-->
<!--*    Generated from PreTeXt source   *-->
<!--*    on 2019-11-11T16:53:53-07:00    *-->
<!--*                                    *-->
<!--*      https://pretextbook.org       *-->
<!--*                                    *-->
<!--**************************************-->
<html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Support Vector Machines</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width,  initial-scale=1.0, user-scalable=0, minimum-scale=1.0, maximum-scale=1.0">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['\\(','\\)']]
    },
    asciimath2jax: {
        ignoreClass: ".*",
        processClass: "has_am"
    },
    jax: ["input/AsciiMath"],
    extensions: ["asciimath2jax.js"],
    TeX: {
        extensions: ["extpfeil.js", "autobold.js", "https://pretextbook.org/js/lib/mathjaxknowl.js", ],
        // scrolling to fragment identifiers is controlled by other Javascript
        positionToHash: false,
        equationNumbers: { autoNumber: "none", useLabelIds: true, },
        TagSide: "right",
        TagIndent: ".8em",
    },
    // HTML-CSS output Jax to be dropped for MathJax 3.0
    "HTML-CSS": {
        scale: 88,
        mtextFontInherit: true,
    },
    CommonHTML: {
        scale: 88,
        mtextFontInherit: true,
    },
});
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML-full"></script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.12/pretext.js"></script><script src="https://pretextbook.org/js/0.12/pretext_add_on.js"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/toc.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/colors_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/setcolors.css" rel="stylesheet" type="text/css">
<!-- 2019-10-12: Temporary - CSS file for experiments with styling --><link href="developer.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/features.css" rel="stylesheet" type="text/css">
<script>var logged_in = false;
var role = 'student';
var guest_access = true;
var login_required = false;
var js_version = 0.12;
</script>
</head>
<body class="mathbook-book has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div class="hidden-content" style="display:none">\(\newcommand{\doubler}[1]{2#1}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\)</div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="datascience.html"><span class="title">Data Science with Python</span></a></h1>
<p class="byline">Virgil U Pierce</p>
</div>
</div></div>
<nav id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3"><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="section-53.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="lda_svm.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="decision_trees.html" title="Next">Next</a></span></div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="section-53.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="lda_svm.html" title="Up">Up</a><a class="next-button button toolbar-item" href="decision_trees.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link"><a href="frontmatter-1.html" data-scroll="frontmatter-1"><span class="title">Front Matter</span></a></li>
<li class="link">
<a href="course_syllabus.html" data-scroll="course_syllabus"><span class="codenumber">1</span> <span class="title">Course Syllabus</span></a><ul>
<li><a href="section-1.html" data-scroll="section-1">Class and Instructor Details</a></li>
<li><a href="section-2.html" data-scroll="section-2">Course Description</a></li>
<li><a href="section-3.html" data-scroll="section-3">Textbook and Software</a></li>
<li><a href="section-4.html" data-scroll="section-4">Learning Objectives / Outcomes for the Course</a></li>
<li><a href="section-5.html" data-scroll="section-5">Communicating</a></li>
<li><a href="section-6.html" data-scroll="section-6">Course Outline</a></li>
<li><a href="section-7.html" data-scroll="section-7">Connections with Industrial Mathematics</a></li>
<li><a href="section-8.html" data-scroll="section-8">Assessments</a></li>
<li><a href="section-9.html" data-scroll="section-9">UNCO Policy Statements</a></li>
</ul>
</li>
<li class="link">
<a href="data_science_introduction.html" data-scroll="data_science_introduction"><span class="codenumber">2</span> <span class="title">Introduction to Data Science</span></a><ul>
<li><a href="section-10.html" data-scroll="section-10">Anaconda, Jupyter, and Python</a></li>
<li><a href="section-11.html" data-scroll="section-11">Github</a></li>
<li><a href="section-12.html" data-scroll="section-12">Python</a></li>
<li><a href="section-13.html" data-scroll="section-13">Some Prelimaries</a></li>
<li><a href="section-14.html" data-scroll="section-14">First Motivating Example - Baseball Players</a></li>
<li><a href="section-15.html" data-scroll="section-15">Second Motivating Example - Abalone Characteristics</a></li>
<li><a href="section-16.html" data-scroll="section-16">Third Motivating Example - US Income Levels</a></li>
<li><a href="section-17.html" data-scroll="section-17">Fourth Motivating Example - Mushroom Characteristics</a></li>
<li><a href="section-18.html" data-scroll="section-18">Fifth Motivating Example - House Prices</a></li>
<li><a href="section-19.html" data-scroll="section-19">Sixth Motivating Example - Running Data from Garmin</a></li>
<li><a href="section-20.html" data-scroll="section-20">Seventh Motivating Example - Berlin Airbnb Data</a></li>
<li><a href="section-21.html" data-scroll="section-21">Eighth Motivating Example - Colorado Child Care</a></li>
<li><a href="section-22.html" data-scroll="section-22">Ninth Motivating Example - Flight Delays at DEN</a></li>
<li><a href="section-23.html" data-scroll="section-23">Tenth Motivating Example - Image Classification</a></li>
<li><a href="section-24.html" data-scroll="section-24">Left for a future class - Unsupervised Learning</a></li>
</ul>
</li>
<li class="link">
<a href="data.html" data-scroll="data"><span class="codenumber">3</span> <span class="title">Data</span></a><ul>
<li><a href="section-25.html" data-scroll="section-25">What is Data</a></li>
<li><a href="section-26.html" data-scroll="section-26">Supervised versus Unsupervised Learning</a></li>
<li><a href="section-27.html" data-scroll="section-27">Where to get Data</a></li>
</ul>
</li>
<li class="link">
<a href="tools.html" data-scroll="tools"><span class="codenumber">4</span> <span class="title">Tools of the Trade</span></a><ul>
<li><a href="section-28.html" data-scroll="section-28">Python and Jupyter</a></li>
<li><a href="section-29.html" data-scroll="section-29">Development</a></li>
<li><a href="section-30.html" data-scroll="section-30">Versioning Control</a></li>
</ul>
</li>
<li class="link">
<a href="process.html" data-scroll="process"><span class="codenumber">5</span> <span class="title">The Data Science Process</span></a><ul>
<li><a href="section-31.html" data-scroll="section-31">Professional Ethics</a></li>
<li><a href="section-32.html" data-scroll="section-32">Controlling for Error</a></li>
<li><a href="section-33.html" data-scroll="section-33">Error in Categorization Problems</a></li>
</ul>
</li>
<li class="link">
<a href="wrangling.html" data-scroll="wrangling"><span class="codenumber">6</span> <span class="title">Wrangling the Data</span></a><ul>
<li><a href="section-34.html" data-scroll="section-34">Formatting the Data</a></li>
<li><a href="section-35.html" data-scroll="section-35">Dealing with Strings</a></li>
<li><a href="section-36.html" data-scroll="section-36">Dealing with Categorical Data</a></li>
<li><a href="section-37.html" data-scroll="section-37">Dealing with Missing Data</a></li>
<li><a href="section-38.html" data-scroll="section-38">Dealing with Images</a></li>
</ul>
</li>
<li class="link">
<a href="resampling.html" data-scroll="resampling"><span class="codenumber">7</span> <span class="title">Resampling</span></a><ul>
<li><a href="section-39.html" data-scroll="section-39">Cross Validation</a></li>
<li><a href="section-40.html" data-scroll="section-40">Bootstraps</a></li>
</ul>
</li>
<li class="link">
<a href="EDA.html" data-scroll="EDA"><span class="codenumber">8</span> <span class="title">Exploratory Data Analysis</span></a><ul><li><a href="section-41.html" data-scroll="section-41">Nonlinear Relations</a></li></ul>
</li>
<li class="link">
<a href="linear_regression.html" data-scroll="linear_regression"><span class="codenumber">9</span> <span class="title">Linear Regression</span></a><ul>
<li><a href="section-42.html" data-scroll="section-42">Calculus Approach to Linear Regression</a></li>
<li><a href="section-43.html" data-scroll="section-43">Linear Regression as Projection</a></li>
</ul>
</li>
<li class="link">
<a href="pca.html" data-scroll="pca"><span class="codenumber">10</span> <span class="title">Principal Component Analysis</span></a><ul>
<li><a href="section-44.html" data-scroll="section-44">Eigenvalue Decomposition of Square Matrix</a></li>
<li><a href="section-45.html" data-scroll="section-45">Singular Value Decomposition</a></li>
</ul>
</li>
<li class="link">
<a href="k-nn.html" data-scroll="k-nn"><span class="codenumber">11</span> <span class="title">k-Nearest Neighbors</span></a><ul>
<li><a href="section-46.html" data-scroll="section-46">Checking Performance with Bootstraps</a></li>
<li><a href="section-47.html" data-scroll="section-47">Normalization</a></li>
<li><a href="section-48.html" data-scroll="section-48">k-Nearest Neighbors with Many Factors</a></li>
<li><a href="section-49.html" data-scroll="section-49">k-Nearest Neighbors for Regression</a></li>
</ul>
</li>
<li class="link">
<a href="ridge_and_lasso.html" data-scroll="ridge_and_lasso"><span class="codenumber">12</span> <span class="title">Ridge and Lasso Regression</span></a><ul>
<li><a href="section-50.html" data-scroll="section-50">House Pricing Data: Linear Regression</a></li>
<li><a href="section-51.html" data-scroll="section-51">Ridge Regression</a></li>
<li><a href="section-52.html" data-scroll="section-52">Lasso Regression</a></li>
</ul>
</li>
<li class="link">
<a href="lda_svm.html" data-scroll="lda_svm"><span class="codenumber">13</span> <span class="title">Linear Discrimant Analysis and Support Vector Machines</span></a><ul>
<li><a href="section-53.html" data-scroll="section-53">Linear Discriminant Analysis</a></li>
<li><a href="section-54.html" data-scroll="section-54" class="active">Support Vector Machines</a></li>
</ul>
</li>
<li class="link">
<a href="decision_trees.html" data-scroll="decision_trees"><span class="codenumber">14</span> <span class="title">Decsion Trees</span></a><ul>
<li><a href="section-55.html" data-scroll="section-55">Regression Trees</a></li>
<li><a href="section-56.html" data-scroll="section-56">Classification Tree</a></li>
<li><a href="section-57.html" data-scroll="section-57">High Dimensional Data and Decision Trees</a></li>
<li><a href="section-58.html" data-scroll="section-58">Discussion of Decision Tree Algorithms</a></li>
</ul>
</li>
<li class="link">
<a href="neural-networks.html" data-scroll="neural-networks"><span class="codenumber">15</span> <span class="title">Neural Networks</span></a><ul>
<li><a href="section-59.html" data-scroll="section-59">Neural Network for Regression</a></li>
<li><a href="section-60.html" data-scroll="section-60">Neural Networks for Classification</a></li>
<li><a href="section-61.html" data-scroll="section-61">Neural Network with a Large Number of Features</a></li>
</ul>
</li>
<li class="link">
<a href="ensemble.html" data-scroll="ensemble"><span class="codenumber">16</span> <span class="title">General Ensemble Models</span></a><ul>
<li><a href="section-62.html" data-scroll="section-62">Why do Ensemble Models Work</a></li>
<li><a href="section-63.html" data-scroll="section-63">Ensemble Models for Regression</a></li>
</ul>
</li>
</ul></nav><div class="extras"><nav><a class="mathbook-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content"><section class="section" id="section-54"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">13.2</span> <span class="title">Support Vector Machines</span>
</h2>
<section class="subsection" id="subsection-72"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">13.2.1</span> <span class="title">Maximal Margin Classifier</span>
</h3>
<p id="p-792">Support vector machines work from a different principle, but one which is motivated by the linear decsion boundaries we are getting in the previous results using <em class="emphasis">Logistic Regression</em> and <em class="emphasis">Linear Discriminant Analysis</em>. To motivate them we start with a classification problem that is relatively rare: one where there is a clear separation in the training data.</p>
<figure class="figure-like" id="listing-348"><pre class="console"><b>iris = pa.read_csv('Data Sets/iris.csv')
iris.head()
</b>   sepal length  sepal width  petal length  petal width  flower type
0           5.1          3.5           1.4          0.2            0
1           4.9          3.0           1.4          0.2            0
2           4.7          3.2           1.3          0.2            0
3           4.6          3.1           1.5          0.2            0
4           5.0          3.6           1.4          0.2            0
</pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">13.2.1.</span> </figcaption></figure><p id="p-793">The flower type 0 is significantly separated from flower types 1 and 0:</p>
<figure class="figure-like" id="listing-349"><pre class="console"><b>X = np.array(iris[['petal length', 'petal width']])
y = np.array(iris['flower type'].map({0:0, 1:1, 2:1}) )
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">13.2.2.</span> </figcaption></figure><figure class="figure-like" id="listing-350"><pre class="console"><b>plt.figure(figsize=(10, 8))
plt.plot(X[y==0, 0], X[y==0, 1], 'r.')
plt.plot(X[y==1, 0], X[y==1, 1], 'g.');
plt.savefig('13.13.png')
</b></pre>
<div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="Images/13.13.png" style="width: 100%; height: auto;" alt=""></div>
<figcaption><span class="type">Listing</span> <span class="codenumber">13.2.3.</span> </figcaption></figure><p id="p-794">In this case we see that there exists at least one (and hence infinitely many) lines that separate the two classes.</p>
<figure class="figure-like" id="listing-351"><pre class="console"><b>xx = np.linspace(0.5, 7.5, 50)
yy = 1.5 - 0.25*xx

plt.figure(figsize=(10, 8))
plt.plot(X[y==0, 0], X[y==0, 1], 'r.')
plt.plot(X[y==1, 0], X[y==1, 1], 'g.')
plt.plot(xx, yy, 'b-');
plt.savefig('13.14.png')
</b></pre>
<div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="Images/13.14.png" style="width: 100%; height: auto;" alt=""></div>
<figcaption><span class="type">Listing</span> <span class="codenumber">13.2.4.</span> </figcaption></figure><p id="p-795">So the goal is to choose the best of all of the possible lines that separate the two classes. Of course, based on the whole class, you are not surprised and are expecting that there are many ideas one might mean by <em class="emphasis">best</em>.</p>
<p id="p-796">One definition would be the dividing line that is as far as possible from both classes. We can define how far the line is from a class as the minimimum perpindicular distance from the points in that class.</p>
<p id="p-797">The distance between the line and the closest training data is called the margin, and intuitively the larger the margin the better we expect the decision boundary to do on new data. Note then that the choice of decision boundary would only be affected by the points that are on the margin. I.e. if a point on the margin moves, it will cause the decision boundary to move; if a point not on the margin moves, so long as it does not move far enough to reach the margin, it will not cause the decision boundary to move. The points on the margine are called the <em class="emphasis">Support Vectors</em>.</p>
<p id="p-798">This gives <em class="emphasis">Support Vector Machines</em>, derived from this idea, an interesting property. They are typically only dependent on a subset of the training data. Which means they are less likely to be affected by errors in some of the training data or changes in the training data, and are less dependent on training data far from the other classes. In the example above this means the Irises in the upper-right coordinate of the graph in particular will not have a strong affect on where we put the division line betweent type 0 and types 1 and 2 irises.</p>
<p id="p-799">The mathematical formulation of this motivating model, called the <em class="emphasis">Maximal Margin Classifier</em> is given by the optimization problem (with class values of \(\{ -1, 1 \}\) rather than the 0 and 1 above:</p>
<div class="displaymath" id="p-800">
\begin{equation*}
\mbox{maximize} \quad M \qquad \mbox{over choices of} \quad \beta_0, \beta_1, \dots \beta_p
\end{equation*}
</div>
<div class="displaymath" id="p-801">
\begin{equation*}
\mbox{subject to} \quad \sum_{j=0}^{p-1} \beta_j^2 = 1
\end{equation*}
</div>
<div class="displaymath" id="p-802">
\begin{equation*}
\mbox{and} \quad y_i(\beta_0 + \beta_1 x_{i 1} + \beta_2 x_{i 2} + \dots + \beta_p x_{i, p} ) \geq M
\end{equation*}
</div>
<p id="p-803">for all \(i\text{.}\)</p></section><section class="subsection" id="subsection-73"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">13.2.2</span> <span class="title">Support Vector Machines</span>
</h3>
<p id="p-804">Now however consider the original data set with three classes.</p>
<figure class="figure-like" id="listing-352"><pre class="console"><b>X = np.array(iris[['petal length', 'petal width']])
y = np.array(iris['flower type'] )
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">13.2.5.</span> </figcaption></figure><figure class="figure-like" id="listing-353"><pre class="console"><b>plt.figure(figsize=(10, 8))
plt.plot(X[y==0, 0], X[y==0, 1], 'r.')
plt.plot(X[y==1, 0], X[y==1, 1], 'g.')
plt.plot(X[y==2, 0], X[y==2, 1], 'b.');
plt.savefig('13.15.png')
</b></pre>
<div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="Images/13.15.png" style="width: 100%; height: auto;" alt=""></div>
<figcaption><span class="type">Listing</span> <span class="codenumber">13.2.6.</span> </figcaption></figure><p id="p-805">In the case of the decision boundary between \(y=1\) and \(y=2\) it is not clear that there exists a linear boundary that will separate the two classes (when I teach this subject as an introductory mathematics course we would spend some time discussing how we might prove that there is in fact no line that separates these two classes, at least from the variables I am using).</p>
<p id="p-806">What we can do is use a standard mathematical trick: <em class="alert">If there is no solution to the problem we want to solve or the solution is hard to find, relax the question.</em></p>
<p id="p-807">This is a mathematical trick made famous by the work of Strassen et al in number theory when they famously changed the question of: <em class="alert">Is n a prime number</em> a very hard problem into <em class="alert">Is n likely to be a prime a number</em> and successfully showed that this problem was tractable. <a class="external" href="https://www-history.mcs.st-andrews.ac.uk/Biographies/Strassen.html" target="_blank">https://www-history.mcs.st-andrews.ac.uk/Biographies/Strassen.html</a></p>
<p id="p-808">In this context it means that what we should do is allow some of the data points from the training set to penetrate the margin or even to cross the decision boundary to the wrong side. That is we should allow some training error, which we have already seen in other models is an essential feature as we try to avoid overfitting.</p>
<p id="p-809">The mathematical formulation above changes to:</p>
<div class="displaymath" id="p-810">
\begin{equation*}
\mbox{maximize} \quad M \qquad \mbox{over choices of} \quad \beta_0, \beta_1, \dots \beta_p
\end{equation*}
</div>
<div class="displaymath" id="p-811">
\begin{equation*}
\mbox{subject to} \quad \sum_{j=0}^{p-1} \beta_j^2 = 1 
\end{equation*}
</div>
<div class="displaymath" id="p-812">
\begin{equation*}
\quad y_i(\beta_0 + \beta_1 x_{i 1} + \beta_2 x_{i 2} + \dots + \beta_p x_{i, p} ) \geq M (1- \epsilon_i)
\end{equation*}
</div>
<p id="p-813">for all \(i\text{;}\) and</p>
<div class="displaymath" id="p-814">
\begin{equation*}
\epsilon_i &gt; 0, \quad \sum_{i=0}^{n-1} \epsilon_i \leq C 
\end{equation*}
</div>
<p id="p-815">where \(C\) is a tuning parameter that identifies how much violation of the margin we will tolerate. How do we choose \(C\text{?}\)</p></section><section class="subsection" id="subsection-74"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">13.2.3</span> <span class="title">Applying Support Vector Machines</span>
</h3>
<figure class="figure-like" id="listing-354"><pre class="console"><b># A bit of code for plotting the decision boundaries in a categorical problem with two predictors and three values for the result
# Also prints out the training and testing errors

def plot_cat_model_3(clf, X_train, y_train, X_test, y_test):

    h = 0.1 # Mesh size  
    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, m_max]x[y_min, y_max].
    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
    y_min, y_max = X_train[:, 1].min() - 2, X_train[:, 1].max() + 2
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
        np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(8, 6))
    cmap_light = pltco.ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
    cmap_bold = pltco.ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
    plt.pcolormesh(xx, yy, Z, cmap=cmap_light);

    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold,marker='o')
    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap_bold, marker='+');
    
    print('Training Error: {}'.format(clf.score(X_train, y_train)))
    print('Testing Error: {}'.format(clf.score(X_test, y_test)))
    
# Updating the plotting function for three categories
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">13.2.7.</span> </figcaption></figure><figure class="figure-like" id="listing-355"><pre class="console"><b># We shuffle the data using a random permutation

n = X.shape[0]
test = int(0.20*n)            # We will use a test set made up of 20% of the data from our sample
perm = rn.permutation(n)   
X = X[perm]
y = y[perm]
X_test = X[:test]       # Then create the test
y_test = y[:test]
X_train = X[test:]     # and train sets
y_train = y[test:]
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">13.2.8.</span> </figcaption></figure><figure class="figure-like" id="listing-356"><pre class="console"><b>from sklearn.svm import SVC
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">13.2.9.</span> </figcaption></figure><figure class="figure-like" id="listing-357"><pre class="console"><b>sv = SVC(C=1, gamma='scale', kernel='linear')
sv.fit(X_train, y_train)
plot_cat_model_3(sv, X_train, y_train, X_test, y_test)
plt.savefig('13.16.png')
</b>Training Error: 0.9583333333333334
Testing Error: 0.9333333333333333
</pre>
<div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="Images/13.16.png" style="width: 100%; height: auto;" alt=""></div>
<figcaption><span class="type">Listing</span> <span class="codenumber">13.2.10.</span> </figcaption></figure><p id="p-816">The model does a very good job of working with the iris problem. Unless one of the problematic points ends up in the testing data we will typically have good performance on the testing set.</p>
<p id="p-817">Returning to the baseball player problem, for which there is more overlap between the classes:</p>
<figure class="figure-like" id="listing-358"><pre class="console"><b>X = np.array(major_2[['Height', 'Weight']])
y = np.array(major_2['Position'])
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">13.2.11.</span> </figcaption></figure><figure class="figure-like" id="listing-359"><pre class="console"><b># We shuffle the data using a random permutation

n = X.shape[0]
test = int(0.20*n)            # We will use a test set made up of 20% of the data from our sample
perm = rn.permutation(n)   
X = X[perm]
y = y[perm]
X_test = X[:test]       # Then create the test
y_test = y[:test]
X_train = X[test:]     # and train sets
y_train = y[test:]
</b></pre>
<figcaption><span class="type">Listing</span> <span class="codenumber">13.2.12.</span> </figcaption></figure><figure class="figure-like" id="listing-360"><pre class="console"><b>sv = SVC(C=2, gamma='scale', kernel='linear')
sv.fit(X_train, y_train)
plot_cat_model(sv, X_train, y_train, X_test, y_test)
plt.savefig('13.17.png')
</b>Training Error: 0.6384522370012092
Testing Error: 0.7087378640776699
</pre>
<div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="Images/13.17.png" style="width: 100%; height: auto;" alt=""></div>
<figcaption><span class="type">Listing</span> <span class="codenumber">13.2.13.</span> </figcaption></figure><p id="p-818">Note the option being used in the model declaration of <em class="emphasis">kernel</em>.  This is an additional generalization of the full <em class="emphasis">Support Vector Machines</em> algorithm which allows us to specify what type of function to use to build the boundary. It can be <em class="emphasis">linear</em>, <em class="emphasis">poly</em>, <em class="emphasis">rbf</em> (for radial basis function, which is mostly what it sounds like), <em class="emphasis">sigmoid</em> (basically a logistic function), <em class="emphasis">precomputed</em> or even callable which would be a Python define class of functions to use for the boundary.</p>
<p id="p-819">How to choose the best kernel to use (other than the last two that can get complicated)?</p>
<figure class="figure-like" id="listing-361"><pre class="console"><b>sv = SVC(C=2, gamma='scale', kernel='poly')
sv.fit(X_train, y_train)
plot_cat_model(sv, X_train, y_train, X_test, y_test)
plt.savefig('13.18.png')
</b>Training Error: 0.6324062877871826
Testing Error: 0.6796116504854369
</pre>
<div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="Images/13.18.png" style="width: 100%; height: auto;" alt=""></div>
<figcaption><span class="type">Listing</span> <span class="codenumber">13.2.14.</span> </figcaption></figure><figure class="figure-like" id="listing-362"><pre class="console"><b>sv = SVC(C=2, gamma='scale', kernel='rbf')
sv.fit(X_train, y_train)
plot_cat_model(sv, X_train, y_train, X_test, y_test)
plt.savefig('13.19.png')
</b>Training Error: 0.6058041112454655
Testing Error: 0.6213592233009708
</pre>
<div class="image-box" style="width: 100%; margin-left: 0%; margin-right: 0%;"><img src="Images/13.19.png" style="width: 100%; height: auto;" alt=""></div>
<figcaption><span class="type">Listing</span> <span class="codenumber">13.2.15.</span> </figcaption></figure><p id="p-820">It is interesting that the curvature from these two kernels is opposite (and disagrees with that from the <em class="emphasis">Quadratic Discriminant Analysis</em>) the performance of all three is not too far apart.</p></section><section class="subsection" id="subsection-75"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">13.2.4</span> <span class="title">Speed</span>
</h3>
<p id="p-821">You might have noticed, if you have been following along in Python, that these algorithms seem to take longer. The training time for the <em class="emphasis">SVC</em> algorithm is definitely one of its disadvantages and for a large dataset will become problematic if you need results fast or are still exploring your problem. See the scikitlearn page for some advice on how to proceed and in particular on the <em class="emphasis">LinearSVC</em> model that will offer some speed improvements over the <em class="emphasis">SVC</em> model.</p>
<p id="p-822">Starting with this class of model, the speed of training a model will become more of a consideration as we go on.</p></section></section></div></main>
</div>
<div class="login-link"><span id="loginlogout" class="login">login</span></div>
<script src="https://pretextbook.org/js/0.12/login.js"></script>
</body>
</html>
